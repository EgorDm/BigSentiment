{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Credit to: https://www.kaggle.com/kyakovlev/preprocessing-bert-public\n",
    "\n",
    "# General imports|  \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, sys, gc, re, warnings, pickle, itertools, emoji, psutil, random, unicodedata, torch\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Initial vars"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "HELPER_PATH             = '../data/helpers/'\n",
    "LOCAL_TEST = False       ## Local test - for test performance on part of the train set only\n",
    "verbose = False\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "EPLACEHOLDER = 'ENTITY'\n",
    "\n",
    "## Load helper helper))\n",
    "def load_helper_file(filename):\n",
    "    with open(HELPER_PATH+filename+'.pickle', 'rb') as f:\n",
    "        temp_obj = pickle.load(f)\n",
    "    return temp_obj\n",
    "\n",
    "## Seeder\n",
    "# :seed to make all processes deterministic     # type: int\n",
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    if 'torch' in sys.modules:\n",
    "        torch.manual_seed(seed)\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Helpers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Multiprocessing Run.\n",
    "# :df - DataFrame to split                      # type: pandas DataFrame\n",
    "# :func - Function to apply on each split       # type: python function\n",
    "# This function is NOT 'bulletproof', be carefull and pass only correct types of variables.\n",
    "def df_parallelize_run(df, func):\n",
    "    num_partitions, num_cores = 16, psutil.cpu_count()  # number of partitions and cores\n",
    "    df_split = np.array_split(df, num_partitions)\n",
    "    pool = Pool(num_cores)\n",
    "    df = pd.concat(pool.map(func, df_split))\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    return df\n",
    "\n",
    "## Build of vocabulary from file - reading data line by line\n",
    "## Line splited by 'space' and we store just first argument - Word\n",
    "# :path - txt/vec/csv absolute file path        # type: str\n",
    "def get_vocabulary(path):\n",
    "    with open(path) as f:\n",
    "        return [line.strip().split()[0] for line in f][0:]\n",
    "\n",
    "## Check how many words are in Vocabulary\n",
    "# :c_list - 1d array with 'comment_text'        # type: pandas Series\n",
    "# :vocabulary - words in vocabulary to check    # type: list of str\n",
    "# :response - type of response                  # type: str\n",
    "def check_vocab(c_list, vocabulary, response='default'):\n",
    "    try:\n",
    "        words = set([w for line in c_list for w in line.split()])\n",
    "        u_list = words.difference(set(vocabulary))\n",
    "        k_list = words.difference(u_list)\n",
    "\n",
    "        if response=='default':\n",
    "            print('Unknown words:', len(u_list), '| Known words:', len(k_list))\n",
    "        elif response=='unknown_list':\n",
    "            return list(u_list)\n",
    "        elif response=='known_list':\n",
    "            return list(k_list)\n",
    "    except:\n",
    "        return []\n",
    "\n",
    "## Simple \"Memory profilers\" to see memory usage\n",
    "def get_memory_usage():\n",
    "    return np.round(psutil.Process(os.getpid()).memory_info()[0]/2.**30, 2)\n",
    "\n",
    "def sizeof_fmt(num, suffix='B'):\n",
    "    for unit in ['','Ki','Mi','Gi','Ti','Pi','Ei','Zi']:\n",
    "        if abs(num) < 1024.0:\n",
    "            return \"%3.1f%s%s\" % (num, unit, suffix)\n",
    "        num /= 1024.0\n",
    "    return \"%.1f%s%s\" % (num, 'Yi', suffix)\n",
    "\n",
    "## Domain Search\n",
    "re_3986_enhanced = re.compile(r\"\"\"\n",
    "        # Parse and capture RFC-3986 Generic URI components.\n",
    "        ^                                    # anchor to beginning of string\n",
    "        (?:  (?P<scheme>    [^:/?#\\s]+):// )?  # capture optional scheme\n",
    "        (?:(?P<authority>  [^/?#\\s]*)  )?  # capture optional authority\n",
    "             (?P<path>        [^?#\\s]*)      # capture required path\n",
    "        (?:\\?(?P<query>        [^#\\s]*)  )?  # capture optional query\n",
    "        (?:\\#(?P<fragment>      [^\\s]*)  )?  # capture optional fragment\n",
    "        $                                    # anchor to end of string\n",
    "        \"\"\", re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "re_domain =  re.compile(r\"\"\"\n",
    "        # Pick out top two levels of DNS domain from authority.\n",
    "        (?P<domain>[^.]+\\.[A-Za-z]{2,6})  # $domain: top two domain levels.\n",
    "        (?::[0-9]*)?                      # Optional port number.\n",
    "        $                                 # Anchor to end of string.\n",
    "        \"\"\",\n",
    "        re.MULTILINE | re.VERBOSE)\n",
    "\n",
    "def domain_search(text):\n",
    "    try:\n",
    "        return re_domain.search(re_3986_enhanced.match(text).group('authority')).group('domain')\n",
    "    except:\n",
    "        return 'url'\n",
    "\n",
    "## Preprocess helpers\n",
    "def place_hold(w):\n",
    "    return WPLACEHOLDER + '['+re.sub(' ', '___', w)+']'\n",
    "\n",
    "def check_replace(w):\n",
    "    return not bool(re.search(WPLACEHOLDER, w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def make_dict_cleaning(s, w_dict, skip_check=False):\n",
    "    if skip_check or check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s\n",
    "\n",
    "def export_dict(temp_dict, serial_num):\n",
    "    pd.DataFrame.from_dict(temp_dict, orient='index').to_csv('dict_'+str(serial_num)+'.csv')\n",
    "\n",
    "def print_dict(temp_dict, n_items=10):\n",
    "    run = 0\n",
    "    for k,v in temp_dict.items():\n",
    "        print(k,'---',v)\n",
    "        run +=1\n",
    "        if run==n_items:\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Get basic helper data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "good_cols       = ['_id', 'text']\n",
    "data = pd.read_csv('../data/bitcoin_twitter_raw.csv')\n",
    "if LOCAL_TEST:\n",
    "    data = data.iloc[:10000][good_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Word / Vocab cleaning"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "texts = data['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "global_lower=True\n",
    "texts = texts.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "if global_lower:\n",
    "    texts = texts.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "# Global\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "texts = texts.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "texts = texts.apply(lambda x: deaccent(x))\n",
    "if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# Remove 'control' chars\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Remove hrefs\n",
    "# Global\n",
    "texts = texts.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_chars)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars = '·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji.UNICODE_EMOJI) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "chars_dict = {}\n",
    "for char in chars:\n",
    "    try:\n",
    "        new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "        if len(new_char)==1:\n",
    "            chars_dict[ord(char)] = new_char\n",
    "        else:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    except:\n",
    "        chars_dict[ord(char)] = ''\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print(chars)\n",
    "if verbose: print_dict(chars_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# Remove html tags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if ('<' in word) and ('>' in word):\n",
    "        for tag in html_tags:\n",
    "            if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                temp_dict[word] = BeautifulSoup(word, 'html5lib').text\n",
    "texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it))\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "\n",
    "for word in temp_dict:\n",
    "    new_value = temp_dict[word]\n",
    "    if word.find('http')>2:\n",
    "        temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value)\n",
    "    else:\n",
    "        temp_dict[word] = place_hold(new_value)\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# Remove twitter links\n",
    "temp_dict = {\n",
    "    'word_placeholder[t.co]': ''\n",
    "}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 1.5:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Remove escaped html\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "symbols = {\n",
    "    '&quot;': '',\n",
    "    '&&amp;': '',\n",
    "    '&lt;': '',\n",
    "    '&gt;': '',\n",
    "}\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if any([rep in word for rep in symbols.keys()]):\n",
    "        new_word = word\n",
    "        for rep, to in symbols.items():\n",
    "            new_word = new_word.replace(rep, to)\n",
    "        temp_dict[word] = new_word\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove escaped html:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# Convert urls part 2\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "\n",
    "for word in temp_vocab:\n",
    "    url_check = False\n",
    "    if 'file:' in word:\n",
    "        url_check = True\n",
    "    elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "        if 'Aww' not in word:\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone in word:\n",
    "                    url_check = True\n",
    "                    break\n",
    "    elif ('/' in word) and ('.' in word):\n",
    "        for d_zone in url_extensions:\n",
    "            if '.' + d_zone + '/' in word:\n",
    "                url_check = True\n",
    "                break\n",
    "\n",
    "    if url_check:\n",
    "        temp_dict[word] =  place_hold(domain_search(word))\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if (pict in word) and (len(pict)>2):\n",
    "                temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "            elif pict==word:\n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# Isolate emoji\n",
    "# Global\n",
    "global_chars_list = list(set([c for line in texts for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if c in emoji.UNICODE_EMOJI])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print(chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "        if (Counter(word)['.']>1):\n",
    "            new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "        if (Counter(word)['!']>1):\n",
    "            new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "        if (Counter(word)['?']>1):\n",
    "            new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "        if (Counter(word)[',']>1):\n",
    "            new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "# Remove underscore for spam words\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "        temp_dict[word] = re.sub('_', '', word)\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# Isolate spam chars repetition\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "        temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(1)])\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "        for pict in pictograms_to_emoji:\n",
    "            if pict==word:\n",
    "                temp_dict[word] = pictograms_to_emoji[pict]\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "# Isolate brakets and quotes\n",
    "# Global\n",
    "chars = '()[]{}<>\"'\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print_dict(chars_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "# Break short words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "# Break long words\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if '_' in word:\n",
    "        temp_dict[word] = re.sub('_', ' ', word)\n",
    "    elif '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "        temp_dict[word] = re.sub('/', ' / ', word)\n",
    "    elif len(' '.join(word.split('-')).split())>2:\n",
    "        temp_dict[word] = re.sub('-', ' ', word)\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "# Remove/Convert usernames and hashtags\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if (len(word) > 3) and (word[1:len(word)-1].replace('_', '').isalnum()):\n",
    "        if not re.compile('[#@$/,.:;]').sub('', word).isnumeric():\n",
    "            if (word.startswith('@')) or (word.startswith('#')):\n",
    "                new_word = place_hold(new_word[0] + new_word[1:])\n",
    "            elif word.startswith('u/'):\n",
    "                new_word = place_hold('@' + new_word[2:])\n",
    "            elif word.startswith('r/'):\n",
    "                new_word = place_hold('#' + new_word[2:])\n",
    "            elif word.startswith('$') and word[1:].isalpha():\n",
    "                new_word = place_hold('#' + new_word[1:])\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[len(word)-1]=='_':\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1]!='_':\n",
    "                new_word = word[:i]\n",
    "                temp_dict[word] = new_word\n",
    "                break\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "# Remove starting underscore\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    if word[0]=='_':\n",
    "        for i in range(len(word)):\n",
    "            if word[i]!='_':\n",
    "                new_word = word[i:]\n",
    "                temp_dict[word] = new_word\n",
    "                break\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "# End word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word),0,-1):\n",
    "        if word[i-1].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# Start word punctuations\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum())]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = word\n",
    "    for i in range(len(word)):\n",
    "        if word[i].isalnum():\n",
    "            new_word = word[:i] + ' ' + word[i:]\n",
    "            break\n",
    "    temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [],
   "source": [
    "# Find and replace acronims\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "        if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "            temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "        else:\n",
    "            if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# Apply spellchecker for contractions\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if word in helper_contractions:\n",
    "        temp_dict[word] = helper_contractions[word] # place_hold(helper_contractions[word])\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "# Local\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "# Convert backslash\n",
    "# Global\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]\n",
    "temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(texts, local_vocab)\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!). TODO check fist against vocab?\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "temp_vocab_dup = []\n",
    "\n",
    "for word in temp_vocab:\n",
    "    if not word.isalpha():\n",
    "        continue\n",
    "    temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "temp_vocab_dup = set(temp_vocab_dup)\n",
    "temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "\n",
    "for word in temp_vocab:\n",
    "    new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "    if new_word in temp_vocab_dup:\n",
    "        temp_dict[word] = new_word\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "# Isolate numbers\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "        if re.compile('[0-9]').sub('', word) != word:\n",
    "            temp_dict[word] = word\n",
    "\n",
    "global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "temp_dict = {k:place_hold(k) for k in temp_dict}\n",
    "\n",
    "#texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Join dashes\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# Try join word (Sloooow)\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "    if (new_word in local_vocab) and (len(new_word)>3):\n",
    "        temp_dict[word] = new_word\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# Try Split word\n",
    "# Local (only unknown words)\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "        chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "        temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion\n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    new_word = convert_leet(word)\n",
    "    if (new_word!=word):\n",
    "        if (len(word)>2) and (new_word in local_vocab):\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [],
   "source": [
    "# Remove placeholders\n",
    "# Global\n",
    "temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "temp_vocab = [k for k in temp_vocab if (not check_replace(k))]\n",
    "temp_dict = {}\n",
    "for word in temp_vocab:\n",
    "    temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "texts = texts.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(texts, local_vocab);\n",
    "if verbose: print_dict(temp_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "data": {
      "text/plain": "         Unnamed: 0           _id  \\\n0                 0  1.357811e+18   \n1                 1  1.358008e+18   \n2                 2  1.358061e+18   \n3                 3  1.358010e+18   \n4                 4  1.358021e+18   \n...             ...           ...   \n11291229    1246654  1.199815e+18   \n11291230    1246655  1.199460e+18   \n11291231    1246656  1.199079e+18   \n11291232    1246657  1.199074e+18   \n11291233    1246658  1.197641e+18   \n\n                                                                                                                                                                                                             text  \\\n0                            blockchains rely on fees to incentivize participation in the decentralized ecosystem . but to get mainstream adoption , we might have to do away with transaction costs altogether .   \n1                                                                                                                                            annual percentage yield ( apy ) #blockchain #cryptocurrency #bitcoin   \n2                                                                                                                                               on a long enough timeline every asset looks flat against #bitcoin   \n3                                                                                                                                                                             #bitcoin breaks $ 40k . . . again !   \n4                                                                                                                                          life gets cheaper on the #bitcoin standard . @jclcapital @crypto_daily   \n...                                                                                                                                                                                                           ...   \n11291229                                              if you feel like this market is winning , take some time to learn to trade this market with us : less than $ 2 per day #crypto #bitcoin #btc #eth #xrp #ltc   \n11291230  please retweet ! signs of a new #bitcoin rally ! small cap #altcoins popping ! #matic + 100 % in 10 days #ripple start up shut down . #ont & amp ; #link partnership cryptgo energy drink launches !...   \n11291231  paperwork gives the exchange time to actually acquire the #bitcoin you bought a claim to . until your bitcoin is in your private wallet , all you have bought is just that . a claim against the exc...   \n11291232  please retweet ! what a difference a day makes ! did #bitcoin find its bottom ? #btc core upgrade ! is the the big reversal ? small cap #altcoins popping ! #ripple invest another $ 20m in #moneygr...   \n11291233                   i am long #btc i will remain macro bullish unless we break the 61 . 8 and make a lower low . . . otherwise this is the perfect buying opportunity going into the halfing #bitcoin #btc   \n\n               user_id follower_count  verified           created_at  \\\n0         2.207129e+09            NaN       NaN  2021-02-05 22:00:14   \n1         1.286718e+18            NaN       NaN  2021-02-06 11:02:10   \n2         1.178503e+18            NaN       NaN  2021-02-06 14:32:47   \n3         1.080703e+18            NaN       NaN  2021-02-06 11:09:37   \n4         9.401471e+17            NaN       NaN  2021-02-06 11:53:50   \n...                ...            ...       ...                  ...   \n11291229  9.739205e+17            NaN       NaN  2019-11-27 22:19:35   \n11291230  9.739205e+17            NaN       NaN  2019-11-26 22:48:52   \n11291231  2.571217e+09            NaN       NaN  2019-11-25 21:35:26   \n11291232  9.739205e+17            NaN       NaN  2019-11-25 21:13:15   \n11291233  3.307664e+09            NaN       NaN  2019-11-21 22:21:38   \n\n         retweet_count  favorite_count  \\\n0                   75           280.0   \n1                    2             5.0   \n2                   52           158.0   \n3                    0             6.0   \n4                    0             0.0   \n...                ...             ...   \n11291229             3             6.0   \n11291230             8            19.0   \n11291231             0             2.0   \n11291232            13            23.0   \n11291233             2            10.0   \n\n                                                  topics  \n0                                                     []  \n1            ['blockchain', 'cryptocurrency', 'bitcoin']  \n2                                            ['bitcoin']  \n3                                            ['bitcoin']  \n4                                            ['bitcoin']  \n...                                                  ...  \n11291229                           ['crypto', 'bitcoin']  \n11291230               ['bitcoin', 'altcoins', 'ripple']  \n11291231                                     ['bitcoin']  \n11291232  ['bitcoin', 'altcoins', 'ripple', 'moneygram']  \n11291233                              ['bitcoin', 'btc']  \n\n[11291234 rows x 10 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Unnamed: 0</th>\n      <th>_id</th>\n      <th>text</th>\n      <th>user_id</th>\n      <th>follower_count</th>\n      <th>verified</th>\n      <th>created_at</th>\n      <th>retweet_count</th>\n      <th>favorite_count</th>\n      <th>topics</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>1.357811e+18</td>\n      <td>blockchains rely on fees to incentivize participation in the decentralized ecosystem . but to get mainstream adoption , we might have to do away with transaction costs altogether .</td>\n      <td>2.207129e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021-02-05 22:00:14</td>\n      <td>75</td>\n      <td>280.0</td>\n      <td>[]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1</td>\n      <td>1.358008e+18</td>\n      <td>annual percentage yield ( apy ) #blockchain #cryptocurrency #bitcoin</td>\n      <td>1.286718e+18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021-02-06 11:02:10</td>\n      <td>2</td>\n      <td>5.0</td>\n      <td>['blockchain', 'cryptocurrency', 'bitcoin']</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>1.358061e+18</td>\n      <td>on a long enough timeline every asset looks flat against #bitcoin</td>\n      <td>1.178503e+18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021-02-06 14:32:47</td>\n      <td>52</td>\n      <td>158.0</td>\n      <td>['bitcoin']</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>3</td>\n      <td>1.358010e+18</td>\n      <td>#bitcoin breaks $ 40k . . . again !</td>\n      <td>1.080703e+18</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021-02-06 11:09:37</td>\n      <td>0</td>\n      <td>6.0</td>\n      <td>['bitcoin']</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>4</td>\n      <td>1.358021e+18</td>\n      <td>life gets cheaper on the #bitcoin standard . @jclcapital @crypto_daily</td>\n      <td>9.401471e+17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2021-02-06 11:53:50</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>['bitcoin']</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>11291229</th>\n      <td>1246654</td>\n      <td>1.199815e+18</td>\n      <td>if you feel like this market is winning , take some time to learn to trade this market with us : less than $ 2 per day #crypto #bitcoin #btc #eth #xrp #ltc</td>\n      <td>9.739205e+17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-11-27 22:19:35</td>\n      <td>3</td>\n      <td>6.0</td>\n      <td>['crypto', 'bitcoin']</td>\n    </tr>\n    <tr>\n      <th>11291230</th>\n      <td>1246655</td>\n      <td>1.199460e+18</td>\n      <td>please retweet ! signs of a new #bitcoin rally ! small cap #altcoins popping ! #matic + 100 % in 10 days #ripple start up shut down . #ont &amp; amp ; #link partnership cryptgo energy drink launches !...</td>\n      <td>9.739205e+17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-11-26 22:48:52</td>\n      <td>8</td>\n      <td>19.0</td>\n      <td>['bitcoin', 'altcoins', 'ripple']</td>\n    </tr>\n    <tr>\n      <th>11291231</th>\n      <td>1246656</td>\n      <td>1.199079e+18</td>\n      <td>paperwork gives the exchange time to actually acquire the #bitcoin you bought a claim to . until your bitcoin is in your private wallet , all you have bought is just that . a claim against the exc...</td>\n      <td>2.571217e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-11-25 21:35:26</td>\n      <td>0</td>\n      <td>2.0</td>\n      <td>['bitcoin']</td>\n    </tr>\n    <tr>\n      <th>11291232</th>\n      <td>1246657</td>\n      <td>1.199074e+18</td>\n      <td>please retweet ! what a difference a day makes ! did #bitcoin find its bottom ? #btc core upgrade ! is the the big reversal ? small cap #altcoins popping ! #ripple invest another $ 20m in #moneygr...</td>\n      <td>9.739205e+17</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-11-25 21:13:15</td>\n      <td>13</td>\n      <td>23.0</td>\n      <td>['bitcoin', 'altcoins', 'ripple', 'moneygram']</td>\n    </tr>\n    <tr>\n      <th>11291233</th>\n      <td>1246658</td>\n      <td>1.197641e+18</td>\n      <td>i am long #btc i will remain macro bullish unless we break the 61 . 8 and make a lower low . . . otherwise this is the perfect buying opportunity going into the halfing #bitcoin #btc</td>\n      <td>3.307664e+09</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>2019-11-21 22:21:38</td>\n      <td>2</td>\n      <td>10.0</td>\n      <td>['bitcoin', 'btc']</td>\n    </tr>\n  </tbody>\n</table>\n<p>11291234 rows × 10 columns</p>\n</div>"
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = texts\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "if not LOCAL_TEST:\n",
    "    data['text'] = texts\n",
    "    data.to_csv('../data/bitcoin_twitter_processed.csv.gz', compression='gzip')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO:\n",
    "* numbers"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
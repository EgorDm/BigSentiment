{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from transformers import ElectraConfig, ElectraForPreTraining, load_tf_weights_in_electra, ElectraForMaskedLM\n",
    "import os\n",
    "import torch\n",
    "from torchviz import make_dot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "MODEL_DIR = '../../data/models/electra_bitcoin_twitter'"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the discriminator model\n",
      "Initialize PyTorch weight ['discriminator_predictions', 'dense', 'bias'] discriminator_predictions/dense/bias\n",
      "Skipping discriminator_predictions/dense/bias/adam_m ['discriminator_predictions', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping discriminator_predictions/dense/bias/adam_v ['discriminator_predictions', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['discriminator_predictions', 'dense', 'kernel'] discriminator_predictions/dense/kernel\n",
      "Skipping discriminator_predictions/dense/kernel/adam_m ['discriminator_predictions', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping discriminator_predictions/dense/kernel/adam_v ['discriminator_predictions', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['discriminator_predictions', 'dense_prediction', 'bias'] discriminator_predictions/dense_1/bias\n",
      "Skipping discriminator_predictions/dense_1/bias/adam_m ['discriminator_predictions', 'dense_prediction', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping discriminator_predictions/dense_1/bias/adam_v ['discriminator_predictions', 'dense_prediction', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['discriminator_predictions', 'dense_prediction', 'kernel'] discriminator_predictions/dense_1/kernel\n",
      "Skipping discriminator_predictions/dense_1/kernel/adam_m ['discriminator_predictions', 'dense_prediction', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping discriminator_predictions/dense_1/kernel/adam_v ['discriminator_predictions', 'dense_prediction', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'beta'] electra/embeddings/LayerNorm/beta\n",
      "Skipping electra/embeddings/LayerNorm/beta/adam_m ['electra', 'embeddings', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/LayerNorm/beta/adam_v ['electra', 'embeddings', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'gamma'] electra/embeddings/LayerNorm/gamma\n",
      "Skipping electra/embeddings/LayerNorm/gamma/adam_m ['electra', 'embeddings', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/LayerNorm/gamma/adam_v ['electra', 'embeddings', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'position_embeddings'] electra/embeddings/position_embeddings\n",
      "Skipping electra/embeddings/position_embeddings/adam_m ['electra', 'embeddings', 'position_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/position_embeddings/adam_v ['electra', 'embeddings', 'position_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'token_type_embeddings'] electra/embeddings/token_type_embeddings\n",
      "Skipping electra/embeddings/token_type_embeddings/adam_m ['electra', 'embeddings', 'token_type_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/token_type_embeddings/adam_v ['electra', 'embeddings', 'token_type_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'word_embeddings'] electra/embeddings/word_embeddings\n",
      "Skipping electra/embeddings/word_embeddings/adam_m ['electra', 'embeddings', 'word_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/word_embeddings/adam_v ['electra', 'embeddings', 'word_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings_project', 'bias'] electra/embeddings_project/bias\n",
      "Skipping electra/embeddings_project/bias/adam_m ['electra', 'embeddings_project', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings_project/bias/adam_v ['electra', 'embeddings_project', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings_project', 'kernel'] electra/embeddings_project/kernel\n",
      "Skipping electra/embeddings_project/kernel/adam_m ['electra', 'embeddings_project', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings_project/kernel/adam_v ['electra', 'embeddings_project', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_0/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_0/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_0/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_0/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_0/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_0/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_0/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_0/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_0/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_0/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_0/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_0/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_0/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias'] electra/encoder/layer_0/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_0/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_0/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_0/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias'] electra/encoder/layer_0/output/dense/bias\n",
      "Skipping electra/encoder/layer_0/output/dense/bias/adam_m ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/output/dense/bias/adam_v ['electra', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel'] electra/encoder/layer_0/output/dense/kernel\n",
      "Skipping electra/encoder/layer_0/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_0/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_1/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_1/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_1/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_1/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_1/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_1/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_1/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_1/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_1/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_1/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_1/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_1/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_1/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias'] electra/encoder/layer_1/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_1/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_1/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_1/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias'] electra/encoder/layer_1/output/dense/bias\n",
      "Skipping electra/encoder/layer_1/output/dense/bias/adam_m ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/output/dense/bias/adam_v ['electra', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel'] electra/encoder/layer_1/output/dense/kernel\n",
      "Skipping electra/encoder/layer_1/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_1/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_10/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_10/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_10/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_10/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_10/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_10/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_10/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_10/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_10/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_10/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_10/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_10/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_10/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias'] electra/encoder/layer_10/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_10/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_10/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_10/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias'] electra/encoder/layer_10/output/dense/bias\n",
      "Skipping electra/encoder/layer_10/output/dense/bias/adam_m ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/output/dense/bias/adam_v ['electra', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel'] electra/encoder/layer_10/output/dense/kernel\n",
      "Skipping electra/encoder/layer_10/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_10/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_11/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_11/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_11/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_11/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_11/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_11/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_11/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_11/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_11/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_11/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_11/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_11/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_11/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias'] electra/encoder/layer_11/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_11/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_11/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_11/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias'] electra/encoder/layer_11/output/dense/bias\n",
      "Skipping electra/encoder/layer_11/output/dense/bias/adam_m ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/output/dense/bias/adam_v ['electra', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel'] electra/encoder/layer_11/output/dense/kernel\n",
      "Skipping electra/encoder/layer_11/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_11/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_2/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_2/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_2/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_2/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_2/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_2/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_2/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_2/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_2/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_2/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_2/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_2/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_2/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias'] electra/encoder/layer_2/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_2/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_2/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_2/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias'] electra/encoder/layer_2/output/dense/bias\n",
      "Skipping electra/encoder/layer_2/output/dense/bias/adam_m ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/output/dense/bias/adam_v ['electra', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel'] electra/encoder/layer_2/output/dense/kernel\n",
      "Skipping electra/encoder/layer_2/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_2/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_3/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_3/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_3/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_3/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_3/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_3/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_3/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_3/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_3/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_3/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_3/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_3/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_3/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias'] electra/encoder/layer_3/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_3/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_3/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_3/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias'] electra/encoder/layer_3/output/dense/bias\n",
      "Skipping electra/encoder/layer_3/output/dense/bias/adam_m ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/output/dense/bias/adam_v ['electra', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel'] electra/encoder/layer_3/output/dense/kernel\n",
      "Skipping electra/encoder/layer_3/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_3/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_4/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_4/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_4/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_4/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_4/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_4/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_4/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_4/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_4/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_4/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_4/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_4/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_4/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias'] electra/encoder/layer_4/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_4/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_4/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_4/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias'] electra/encoder/layer_4/output/dense/bias\n",
      "Skipping electra/encoder/layer_4/output/dense/bias/adam_m ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/output/dense/bias/adam_v ['electra', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel'] electra/encoder/layer_4/output/dense/kernel\n",
      "Skipping electra/encoder/layer_4/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_4/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_5/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_5/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_5/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_5/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_5/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_5/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_5/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_5/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_5/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_5/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_5/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_5/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_5/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias'] electra/encoder/layer_5/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_5/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_5/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_5/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias'] electra/encoder/layer_5/output/dense/bias\n",
      "Skipping electra/encoder/layer_5/output/dense/bias/adam_m ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/output/dense/bias/adam_v ['electra', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel'] electra/encoder/layer_5/output/dense/kernel\n",
      "Skipping electra/encoder/layer_5/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_5/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_6/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_6/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_6/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_6/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_6/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_6/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_6/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_6/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_6/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_6/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_6/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_6/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_6/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias'] electra/encoder/layer_6/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_6/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_6/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_6/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias'] electra/encoder/layer_6/output/dense/bias\n",
      "Skipping electra/encoder/layer_6/output/dense/bias/adam_m ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/output/dense/bias/adam_v ['electra', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel'] electra/encoder/layer_6/output/dense/kernel\n",
      "Skipping electra/encoder/layer_6/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_6/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_7/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_7/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_7/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_7/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_7/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_7/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_7/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_7/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_7/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_7/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_7/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_7/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_7/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias'] electra/encoder/layer_7/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_7/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_7/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_7/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias'] electra/encoder/layer_7/output/dense/bias\n",
      "Skipping electra/encoder/layer_7/output/dense/bias/adam_m ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/output/dense/bias/adam_v ['electra', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel'] electra/encoder/layer_7/output/dense/kernel\n",
      "Skipping electra/encoder/layer_7/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_7/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_8/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_8/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_8/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_8/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_8/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_8/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_8/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_8/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_8/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_8/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_8/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_8/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_8/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias'] electra/encoder/layer_8/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_8/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_8/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_8/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias'] electra/encoder/layer_8/output/dense/bias\n",
      "Skipping electra/encoder/layer_8/output/dense/bias/adam_m ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/output/dense/bias/adam_v ['electra', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel'] electra/encoder/layer_8/output/dense/kernel\n",
      "Skipping electra/encoder/layer_8/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_8/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_9/attention/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_9/attention/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias'] electra/encoder/layer_9/attention/output/dense/bias\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel'] electra/encoder/layer_9/attention/output/dense/kernel\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias'] electra/encoder/layer_9/attention/self/key/bias\n",
      "Skipping electra/encoder/layer_9/attention/self/key/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel'] electra/encoder/layer_9/attention/self/key/kernel\n",
      "Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias'] electra/encoder/layer_9/attention/self/query/bias\n",
      "Skipping electra/encoder/layer_9/attention/self/query/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel'] electra/encoder/layer_9/attention/self/query/kernel\n",
      "Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias'] electra/encoder/layer_9/attention/self/value/bias\n",
      "Skipping electra/encoder/layer_9/attention/self/value/bias/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/bias/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel'] electra/encoder/layer_9/attention/self/value/kernel\n",
      "Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_m ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_v ['electra', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias'] electra/encoder/layer_9/intermediate/dense/bias\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_m ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_v ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel'] electra/encoder/layer_9/intermediate/dense/kernel\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_m ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_v ['electra', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta'] electra/encoder/layer_9/output/LayerNorm/beta\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_m ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_v ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma'] electra/encoder/layer_9/output/LayerNorm/gamma\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_m ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_v ['electra', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias'] electra/encoder/layer_9/output/dense/bias\n",
      "Skipping electra/encoder/layer_9/output/dense/bias/adam_m ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/output/dense/bias/adam_v ['electra', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel'] electra/encoder/layer_9/output/dense/kernel\n",
      "Skipping electra/encoder/layer_9/output/dense/kernel/adam_m ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/encoder/layer_9/output/dense/kernel/adam_v ['electra', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Skipping generator/embeddings_project/bias ['generator', 'embeddings_project', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/embeddings_project/bias/adam_m ['generator', 'embeddings_project', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/embeddings_project/bias/adam_v ['generator', 'embeddings_project', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/embeddings_project/kernel ['generator', 'embeddings_project', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/embeddings_project/kernel/adam_m ['generator', 'embeddings_project', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/embeddings_project/kernel/adam_v ['generator', 'embeddings_project', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/dense/bias ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/dense/kernel ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/key/bias ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/key/kernel ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/query/bias ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/query/kernel ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/value/bias ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/value/kernel ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/intermediate/dense/bias ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/intermediate/dense/kernel ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/LayerNorm/beta ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/LayerNorm/gamma ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/dense/bias ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/dense/bias/adam_m ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/dense/bias/adam_v ['generator', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/dense/kernel ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_0/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/dense/bias ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/dense/kernel ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/key/bias ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/key/kernel ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/query/bias ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/query/kernel ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/value/bias ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/value/kernel ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/intermediate/dense/bias ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/intermediate/dense/kernel ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/LayerNorm/beta ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/LayerNorm/gamma ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/dense/bias ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/dense/bias/adam_m ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/dense/bias/adam_v ['generator', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/dense/kernel ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_1/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/dense/bias ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/dense/kernel ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/key/bias ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/key/kernel ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/query/bias ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/query/kernel ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/value/bias ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/value/kernel ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/intermediate/dense/bias ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/intermediate/dense/kernel ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/LayerNorm/beta ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/LayerNorm/gamma ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/dense/bias ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/dense/bias/adam_m ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/dense/bias/adam_v ['generator', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/dense/kernel ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_10/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/dense/bias ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/dense/kernel ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/key/bias ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/key/kernel ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/query/bias ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/query/kernel ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/value/bias ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/value/kernel ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/intermediate/dense/bias ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/intermediate/dense/kernel ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/LayerNorm/beta ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/LayerNorm/gamma ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/dense/bias ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/dense/bias/adam_m ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/dense/bias/adam_v ['generator', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/dense/kernel ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_11/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/dense/bias ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/dense/kernel ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/key/bias ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/key/kernel ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/query/bias ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/query/kernel ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/value/bias ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/value/kernel ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/intermediate/dense/bias ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/intermediate/dense/kernel ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/LayerNorm/beta ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/LayerNorm/gamma ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/dense/bias ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/dense/bias/adam_m ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/dense/bias/adam_v ['generator', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/dense/kernel ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_2/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/dense/bias ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/dense/kernel ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/key/bias ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/key/kernel ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/query/bias ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/query/kernel ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/value/bias ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/value/kernel ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/intermediate/dense/bias ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/intermediate/dense/kernel ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/LayerNorm/beta ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/LayerNorm/gamma ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/dense/bias ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/dense/bias/adam_m ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/dense/bias/adam_v ['generator', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/dense/kernel ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_3/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/dense/bias ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/dense/kernel ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/key/bias ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/key/kernel ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/query/bias ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/query/kernel ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/value/bias ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/value/kernel ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/intermediate/dense/bias ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/intermediate/dense/kernel ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/LayerNorm/beta ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/LayerNorm/gamma ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/dense/bias ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/dense/bias/adam_m ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/dense/bias/adam_v ['generator', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/dense/kernel ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_4/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/dense/bias ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/dense/kernel ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/key/bias ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/key/kernel ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/query/bias ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/query/kernel ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/value/bias ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/value/kernel ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/intermediate/dense/bias ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/intermediate/dense/kernel ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/LayerNorm/beta ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/LayerNorm/gamma ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/dense/bias ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/dense/bias/adam_m ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/dense/bias/adam_v ['generator', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/dense/kernel ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_5/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/dense/bias ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/dense/kernel ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/key/bias ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/key/kernel ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/query/bias ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/query/kernel ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/value/bias ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/value/kernel ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/intermediate/dense/bias ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/intermediate/dense/kernel ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/LayerNorm/beta ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/LayerNorm/gamma ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/dense/bias ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/dense/bias/adam_m ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/dense/bias/adam_v ['generator', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/dense/kernel ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_6/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/dense/bias ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/dense/kernel ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/key/bias ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/key/kernel ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/query/bias ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/query/kernel ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/value/bias ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/value/kernel ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/intermediate/dense/bias ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/intermediate/dense/kernel ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/LayerNorm/beta ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/LayerNorm/gamma ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/dense/bias ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/dense/bias/adam_m ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/dense/bias/adam_v ['generator', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/dense/kernel ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_7/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/dense/bias ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/dense/kernel ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/key/bias ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/key/kernel ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/query/bias ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/query/kernel ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/value/bias ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/value/kernel ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/intermediate/dense/bias ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/intermediate/dense/kernel ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/LayerNorm/beta ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/LayerNorm/gamma ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/dense/bias ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/dense/bias/adam_m ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/dense/bias/adam_v ['generator', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/dense/kernel ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_8/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/dense/bias ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/dense/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/dense/kernel ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/key/bias ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/key/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/key/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/key/kernel ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/key/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/query/bias ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/query/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/query/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/query/kernel ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/query/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/value/bias ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/value/bias/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/value/bias/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/value/kernel ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_m ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/attention/self/value/kernel/adam_v ['generator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/intermediate/dense/bias ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_m ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/intermediate/dense/bias/adam_v ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/intermediate/dense/kernel ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_m ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/intermediate/dense/kernel/adam_v ['generator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/LayerNorm/beta ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_m ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/LayerNorm/beta/adam_v ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/LayerNorm/gamma ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_m ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/LayerNorm/gamma/adam_v ['generator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/dense/bias ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/dense/bias/adam_m ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/dense/bias/adam_v ['generator', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/dense/kernel ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/dense/kernel/adam_m ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator/encoder/layer_9/output/dense/kernel/adam_v ['generator', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator'\n",
      "Skipping generator_predictions/LayerNorm/beta ['generator_predictions', 'LayerNorm', 'beta'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/LayerNorm/beta/adam_m ['generator_predictions', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/LayerNorm/beta/adam_v ['generator_predictions', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/LayerNorm/gamma ['generator_predictions', 'LayerNorm', 'gamma'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/LayerNorm/gamma/adam_m ['generator_predictions', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/LayerNorm/gamma/adam_v ['generator_predictions', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/dense/bias ['generator_predictions', 'dense', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/dense/bias/adam_m ['generator_predictions', 'dense', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/dense/bias/adam_v ['generator_predictions', 'dense', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/dense/kernel ['generator_predictions', 'dense', 'kernel'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/dense/kernel/adam_m ['generator_predictions', 'dense', 'kernel', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/dense/kernel/adam_v ['generator_predictions', 'dense', 'kernel', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_predictions'\n",
      "Skipping generator_predictions/output_bias ['generator_lm_head', 'bias'] 'ElectraForPreTraining' object has no attribute 'generator_lm_head'\n",
      "Skipping generator_predictions/output_bias/adam_m ['generator_lm_head', 'bias', 'adam_m'] 'ElectraForPreTraining' object has no attribute 'generator_lm_head'\n",
      "Skipping generator_predictions/output_bias/adam_v ['generator_lm_head', 'bias', 'adam_v'] 'ElectraForPreTraining' object has no attribute 'generator_lm_head'\n"
     ]
    },
    {
     "data": {
      "text/plain": "ElectraForPreTraining(\n  (electra): ElectraModel(\n    (embeddings): ElectraEmbeddings(\n      (word_embeddings): Embedding(16537, 128, padding_idx=0)\n      (position_embeddings): Embedding(512, 128)\n      (token_type_embeddings): Embedding(2, 128)\n      (LayerNorm): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (embeddings_project): Linear(in_features=128, out_features=256, bias=True)\n    (encoder): ElectraEncoder(\n      (layer): ModuleList(\n        (0): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): ElectraLayer(\n          (attention): ElectraAttention(\n            (self): ElectraSelfAttention(\n              (query): Linear(in_features=256, out_features=256, bias=True)\n              (key): Linear(in_features=256, out_features=256, bias=True)\n              (value): Linear(in_features=256, out_features=256, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): ElectraSelfOutput(\n              (dense): Linear(in_features=256, out_features=256, bias=True)\n              (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): ElectraIntermediate(\n            (dense): Linear(in_features=256, out_features=1024, bias=True)\n          )\n          (output): ElectraOutput(\n            (dense): Linear(in_features=1024, out_features=256, bias=True)\n            (LayerNorm): LayerNorm((256,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (discriminator_predictions): ElectraDiscriminatorPredictions(\n    (dense): Linear(in_features=256, out_features=256, bias=True)\n    (dense_prediction): Linear(in_features=256, out_features=1, bias=True)\n  )\n)"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CONFIG_FILE = os.path.join(MODEL_DIR, 'config.json')\n",
    "config = ElectraConfig.from_pretrained(CONFIG_FILE)\n",
    "\n",
    "print('Converting the discriminator model')\n",
    "DUMP_PATH = os.path.join(MODEL_DIR, 'discriminator_model.bin')\n",
    "model = ElectraForPreTraining(config)\n",
    "load_tf_weights_in_electra(model, config, MODEL_DIR)\n",
    "\n",
    "print('Saving the model')\n",
    "torch.save(model.state_dict(), DUMP_PATH)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the generator model\n",
      "Skipping discriminator_predictions/dense/bias ['discriminator_predictions', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense/bias/adam_m ['discriminator_predictions', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense/bias/adam_v ['discriminator_predictions', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense/kernel ['discriminator_predictions', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense/kernel/adam_m ['discriminator_predictions', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense/kernel/adam_v ['discriminator_predictions', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense_1/bias ['discriminator_predictions', 'dense_prediction', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense_1/bias/adam_m ['discriminator_predictions', 'dense_prediction', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense_1/bias/adam_v ['discriminator_predictions', 'dense_prediction', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense_1/kernel ['discriminator_predictions', 'dense_prediction', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense_1/kernel/adam_m ['discriminator_predictions', 'dense_prediction', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Skipping discriminator_predictions/dense_1/kernel/adam_v ['discriminator_predictions', 'dense_prediction', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator_predictions'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'beta'] electra/embeddings/LayerNorm/beta\n",
      "Skipping electra/embeddings/LayerNorm/beta/adam_m ['electra', 'embeddings', 'LayerNorm', 'beta', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/LayerNorm/beta/adam_v ['electra', 'embeddings', 'LayerNorm', 'beta', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'LayerNorm', 'gamma'] electra/embeddings/LayerNorm/gamma\n",
      "Skipping electra/embeddings/LayerNorm/gamma/adam_m ['electra', 'embeddings', 'LayerNorm', 'gamma', 'adam_m'] 'Parameter' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/LayerNorm/gamma/adam_v ['electra', 'embeddings', 'LayerNorm', 'gamma', 'adam_v'] 'Parameter' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'position_embeddings'] electra/embeddings/position_embeddings\n",
      "Skipping electra/embeddings/position_embeddings/adam_m ['electra', 'embeddings', 'position_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/position_embeddings/adam_v ['electra', 'embeddings', 'position_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'token_type_embeddings'] electra/embeddings/token_type_embeddings\n",
      "Skipping electra/embeddings/token_type_embeddings/adam_m ['electra', 'embeddings', 'token_type_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/token_type_embeddings/adam_v ['electra', 'embeddings', 'token_type_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
      "Initialize PyTorch weight ['electra', 'embeddings', 'word_embeddings'] electra/embeddings/word_embeddings\n",
      "Skipping electra/embeddings/word_embeddings/adam_m ['electra', 'embeddings', 'word_embeddings', 'adam_m'] 'Embedding' object has no attribute 'adam_m'\n",
      "Skipping electra/embeddings/word_embeddings/adam_v ['electra', 'embeddings', 'word_embeddings', 'adam_v'] 'Embedding' object has no attribute 'adam_v'\n",
      "Skipping electra/embeddings_project/bias ['discriminator', 'embeddings_project', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/embeddings_project/bias/adam_m ['discriminator', 'embeddings_project', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/embeddings_project/bias/adam_v ['discriminator', 'embeddings_project', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/embeddings_project/kernel ['discriminator', 'embeddings_project', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/embeddings_project/kernel/adam_m ['discriminator', 'embeddings_project', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/embeddings_project/kernel/adam_v ['discriminator', 'embeddings_project', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/bias ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/bias ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/kernel ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/bias ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/kernel ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/bias ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/kernel ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_0', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/bias ['discriminator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_0', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_0', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_0', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/dense/bias ['discriminator', 'encoder', 'layer_0', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_0', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/dense/kernel ['discriminator', 'encoder', 'layer_0', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_0/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_0', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/bias ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/bias ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/kernel ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/bias ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/kernel ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/bias ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/kernel ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_1', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/bias ['discriminator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_1', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_1', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_1', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/dense/bias ['discriminator', 'encoder', 'layer_1', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_1', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/dense/kernel ['discriminator', 'encoder', 'layer_1', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_1/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_1', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/bias ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/bias ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/kernel ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/bias ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/kernel ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/bias ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/kernel ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_10', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/bias ['discriminator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_10', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_10', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_10', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/dense/bias ['discriminator', 'encoder', 'layer_10', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_10', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/dense/kernel ['discriminator', 'encoder', 'layer_10', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_10/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_10', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/bias ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/bias ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/kernel ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/bias ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/kernel ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/bias ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/kernel ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_11', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/bias ['discriminator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_11', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_11', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_11', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/dense/bias ['discriminator', 'encoder', 'layer_11', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_11', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/dense/kernel ['discriminator', 'encoder', 'layer_11', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_11/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_11', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/bias ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/bias ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/kernel ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/bias ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/kernel ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/bias ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/kernel ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_2', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/bias ['discriminator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_2', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_2', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_2', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/dense/bias ['discriminator', 'encoder', 'layer_2', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_2', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/dense/kernel ['discriminator', 'encoder', 'layer_2', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_2/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_2', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/bias ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/bias ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/kernel ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/bias ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/kernel ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/bias ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/kernel ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_3', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/bias ['discriminator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_3', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_3', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_3', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/dense/bias ['discriminator', 'encoder', 'layer_3', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_3', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/dense/kernel ['discriminator', 'encoder', 'layer_3', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_3/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_3', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/bias ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/bias ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/kernel ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/bias ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/kernel ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/bias ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/kernel ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_4', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/bias ['discriminator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_4', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_4', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_4', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/dense/bias ['discriminator', 'encoder', 'layer_4', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_4', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/dense/kernel ['discriminator', 'encoder', 'layer_4', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_4/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_4', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/bias ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/bias ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/kernel ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/bias ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/kernel ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/bias ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/kernel ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_5', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/bias ['discriminator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_5', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_5', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_5', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/dense/bias ['discriminator', 'encoder', 'layer_5', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_5', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/dense/kernel ['discriminator', 'encoder', 'layer_5', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_5/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_5', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/bias ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/bias ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/kernel ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/bias ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/kernel ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/bias ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/kernel ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_6', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/bias ['discriminator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_6', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_6', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_6', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/dense/bias ['discriminator', 'encoder', 'layer_6', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_6', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/dense/kernel ['discriminator', 'encoder', 'layer_6', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_6/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_6', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/bias ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/bias ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/kernel ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/bias ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/kernel ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/bias ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/kernel ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_7', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/bias ['discriminator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_7', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_7', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_7', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/dense/bias ['discriminator', 'encoder', 'layer_7', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_7', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/dense/kernel ['discriminator', 'encoder', 'layer_7', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_7/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_7', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/bias ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/bias ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/kernel ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/bias ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/kernel ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/bias ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/kernel ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_8', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/bias ['discriminator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_8', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_8', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_8', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/dense/bias ['discriminator', 'encoder', 'layer_8', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_8', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/dense/kernel ['discriminator', 'encoder', 'layer_8', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_8/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_8', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/bias ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/kernel ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/bias ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/bias/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/bias/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/kernel ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/key/kernel/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'key', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/bias ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/bias/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/bias/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/kernel ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/query/kernel/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'query', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/bias ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/bias/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/bias/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/kernel ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_m ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/attention/self/value/kernel/adam_v ['discriminator', 'encoder', 'layer_9', 'attention', 'self', 'value', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/bias ['discriminator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_m ['discriminator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/bias/adam_v ['discriminator', 'encoder', 'layer_9', 'intermediate', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/kernel ['discriminator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/intermediate/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_9', 'intermediate', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/beta ['discriminator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_m ['discriminator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/beta/adam_v ['discriminator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'beta', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/gamma ['discriminator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_m ['discriminator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/LayerNorm/gamma/adam_v ['discriminator', 'encoder', 'layer_9', 'output', 'LayerNorm', 'gamma', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/dense/bias ['discriminator', 'encoder', 'layer_9', 'output', 'dense', 'bias'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/dense/bias/adam_m ['discriminator', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/dense/bias/adam_v ['discriminator', 'encoder', 'layer_9', 'output', 'dense', 'bias', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/dense/kernel ['discriminator', 'encoder', 'layer_9', 'output', 'dense', 'kernel'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/dense/kernel/adam_m ['discriminator', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_m'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n",
      "Skipping electra/encoder/layer_9/output/dense/kernel/adam_v ['discriminator', 'encoder', 'layer_9', 'output', 'dense', 'kernel', 'adam_v'] 'ElectraForMaskedLM' object has no attribute 'discriminator'\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "('Pointer shape torch.Size([256]) and array shape (64,) mismatched', torch.Size([256]), (64,))",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-18-c9254f986aaa>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mDUMP_PATH\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mMODEL_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m'generator_model.bin'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[0mgenerator_model\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mElectraForMaskedLM\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 7\u001B[0;31m \u001B[0mload_tf_weights_in_electra\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgenerator_model\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mMODEL_DIR\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdiscriminator_or_generator\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;34m'generator'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      8\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/dd_volume/Development/Python/BigHead/venv/lib/python3.8/site-packages/transformers/models/electra/modeling_electra.py\u001B[0m in \u001B[0;36mload_tf_weights_in_electra\u001B[0;34m(model, config, tf_checkpoint_path, discriminator_or_generator)\u001B[0m\n\u001B[1;32m    138\u001B[0m                 \u001B[0marray\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtranspose\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0marray\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    139\u001B[0m             \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 140\u001B[0;31m                 assert (\n\u001B[0m\u001B[1;32m    141\u001B[0m                     \u001B[0mpointer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0marray\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mshape\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    142\u001B[0m                 ), f\"Pointer shape {pointer.shape} and array shape {array.shape} mismatched\"\n",
      "\u001B[0;31mAssertionError\u001B[0m: ('Pointer shape torch.Size([256]) and array shape (64,) mismatched', torch.Size([256]), (64,))"
     ]
    }
   ],
   "source": [
    "CONFIG_FILE = os.path.join(MODEL_DIR, 'config.json')\n",
    "config = ElectraConfig.from_pretrained(CONFIG_FILE)\n",
    "\n",
    "print('Converting the generator model')\n",
    "DUMP_PATH = os.path.join(MODEL_DIR, 'generator_model.bin')\n",
    "generator_model = ElectraForMaskedLM(config)\n",
    "# load_tf_weights_in_electra(generator_model, config, MODEL_DIR, discriminator_or_generator='generator')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "ename": "ModuleAttributeError",
     "evalue": "'ElectraForMaskedLM' object has no attribute 'grad_fn'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleAttributeError\u001B[0m                      Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-20-cf25baf22825>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0mmake_dot\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mgenerator_model\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m/dd_volume/Development/Python/BigHead/venv/lib/python3.8/site-packages/torchviz/dot.py\u001B[0m in \u001B[0;36mmake_dot\u001B[0;34m(var, params)\u001B[0m\n\u001B[1;32m     35\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0;34m'('\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0;34m', '\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mjoin\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'%d'\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;34m+\u001B[0m \u001B[0;34m')'\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     36\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 37\u001B[0;31m     \u001B[0moutput_nodes\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mvar\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrad_fn\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0misinstance\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvar\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32melse\u001B[0m \u001B[0mtuple\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mv\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgrad_fn\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0mv\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mvar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     38\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     39\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0madd_nodes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mvar\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/dd_volume/Development/Python/BigHead/venv/lib/python3.8/site-packages/torch/nn/modules/module.py\u001B[0m in \u001B[0;36m__getattr__\u001B[0;34m(self, name)\u001B[0m\n\u001B[1;32m    776\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mname\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    777\u001B[0m                 \u001B[0;32mreturn\u001B[0m \u001B[0mmodules\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mname\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 778\u001B[0;31m         raise ModuleAttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001B[0m\u001B[1;32m    779\u001B[0m             type(self).__name__, name))\n\u001B[1;32m    780\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mModuleAttributeError\u001B[0m: 'ElectraForMaskedLM' object has no attribute 'grad_fn'"
     ]
    }
   ],
   "source": [
    "make_dot(generator_model)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Credit for some parts to: https://www.kaggle.com/kyakovlev/preprocessing-bert-public\n",
    "# Number extraction and hashtags is my baby\n",
    "\n",
    "# General imports|  \n",
    "import pandas as pd\n",
    "import re, warnings, pickle, itertools, emoji, unicodedata\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from utils.datasets import *\n",
    "from pandarallel import pandarallel\n",
    "import fasttext\n",
    "\n",
    "pandarallel.initialize()\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "outputs": [],
   "source": [
    "## Initial vars\n",
    "\n",
    "HELPER_PATH             = '../../data/helpers/'\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "verbose = True\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "URL_TAG = '@URL'\n",
    "USER_TAG = '@USR'\n",
    "NUMBER_TAG = '@NUM'\n",
    "HASH_TAG = '@HTAG'\n",
    "CURRENCY_TAG = '@CURR'\n",
    "TIME_TAG = '@TIME'\n",
    "DATE_TAG = '@DATE'\n",
    "IMMUTABLES = [\n",
    "    WPLACEHOLDER,\n",
    "    URL_TAG, USER_TAG, NUMBER_TAG, HASH_TAG, CURRENCY_TAG,\n",
    "    TIME_TAG, DATE_TAG\n",
    "]\n",
    "\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "outputs": [],
   "source": [
    "## Preprocess helpers\n",
    "def place_hold(w, tag=WPLACEHOLDER):\n",
    "    return tag + '[' + re.sub(' ', '___', w) + ']'\n",
    "\n",
    "## Helpers\n",
    "def check_replace(w):\n",
    "    return not bool(re.search('|'.join(IMMUTABLES), w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def make_dict_cleaning(s, w_dict, skip_check=False):\n",
    "    # Replaces a word using dict if it is mutable\n",
    "    if skip_check or check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "outputs": [],
   "source": [
    "## Get basic helper data\n",
    "\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "helper_custom_synonyms     = load_helper_file('helper_custom_synonyms')\n",
    "helper_currency_synonyms     = load_helper_file('helper_currency_synonyms')\n",
    "helper_custom_general_synonyms     = load_helper_file('helper_custom_general_synonyms')\n",
    "emoji_dict = set(e for lang in emoji.UNICODE_EMOJI.values() for e in lang)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "outputs": [],
   "source": [
    "## Load Data\n",
    "good_cols       = ['_id', 'text']\n",
    "data = pd.read_parquet('../../data/bitcoin_twitter_raw/part_0.parquet')\n",
    "data = data.iloc[:20000][good_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 63451 | Known words: 6880\n"
     ]
    }
   ],
   "source": [
    "## Start preprocessing\n",
    "texts = data['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "global_lower=True\n",
    "texts = texts.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 54216 | Known words: 7938\n"
     ]
    }
   ],
   "source": [
    "def lower(texts):\n",
    "    texts = texts.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "if global_lower:\n",
    "    texts = texts.pipe(lower)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 53957 | Known words: 7946\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "def normalize_chars(texts):\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "    texts = texts.apply(lambda x: deaccent(x))\n",
    "    if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 553,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 53957 | Known words: 7946\n"
     ]
    }
   ],
   "source": [
    "def remove_control_chars(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_control_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 53957 | Known words: 7946\n"
     ]
    }
   ],
   "source": [
    "def remove_hrefs(texts):\n",
    "    texts = texts.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "    if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_hrefs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 53826 | Known words: 7956\n",
      "ㅜ🅴아⁠𝐒𝖋𝖎𝐨【🇲❯🆂까𝖕스𝟵友ᵛ𝖞𝐂🅽㆔𝟲𝐀권󠁢𝑼𝒕留길寒➤로반🇻🇮다ƀ𝒊𝒂𝐬𝐥🇩ด𝖓𝖑貨비🇬🅻𝐞ꮆ想𝒆ѵ𝐄𝟠데แ商特务내𝐮𝑲탑₦리块⟶🇴🇨ꮇ𝖗트𝖔󠁳円𝑳值ผ𝒔𝒓🇦ค익🇵ๆ지기‌ไ𝟎도포¯⋯⃣🅷跌𝖉𝒉ช더货𝒏랬吴🅳는𝖆𝟰𝐠🇿면에서►模𝟏✓󠁿碳줍炮█٪𝐛𝐯정涨￼🇷𝒌₺𝑾렇】바𝟭🇭𝒐￥𝑻₿链🇳𝟙؟🇰𝖙台ꮤ通⟠🇹𝒅𝐡🅼인려𝖘密条▓了회𝖚交코𝖈𝖊⁦▴시𝒄陆𝟔𝐅𝒎𝟘₳＄ะ𝟓🆃🇺𝕮행🇽ㅠ󠁣𝐭𝟬중션🇸𝐚จ🇧币🇪ข𝕽𝟚⁩니忌나🇱价그수󠁧𝐝𝐦가𝐫⋰덕约‍𝒗฿ⓜ𝒍󠁴░\n",
      "12636 --- u\n",
      "127348 --- e\n",
      "50500 --- a\n",
      "8288 --- \n",
      "119826 --- s\n",
      "120203 --- f\n",
      "120206 --- i\n",
      "119848 --- o\n",
      "12304 --- \n",
      "127474 --- m\n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "def convert_remove_bad_symbols(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji_dict) and (c not in white_list_chars)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 53659 | Known words: 7949\n",
      "·ˢξك→ต…ノي≥√₹п●•ف∞ンทся比چш大ھیу上कثاวمчجヒーыرาअءм下《نタф›دوظน安هتβトقカเพ平区лоـ이لब加и☆。चルд€，यپ„生حцж≈بصضล》аک学ںہยツッвٹाอт？эιมذрюコбگе！عькнπзةгس仮خกشイ\n",
      "183 --- \n",
      "738 --- s\n",
      "958 --- \n",
      "1603 --- \n",
      "8594 --- \n",
      "3605 --- \n",
      "8230 --- \n",
      "12494 --- \n",
      "1610 --- \n",
      "8805 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "def convert_remove_bad_symbols2(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = '·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji_dict) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 53659 | Known words: 7949\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if ('<' in word) and ('>' in word):\n",
    "            for tag in html_tags:\n",
    "                if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                    temp_dict[word] = BeautifulSoup(word, 'html5lib').text\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_html_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 39204 | Known words: 7949\n",
      "https://t.co/rnoz5w1qci --- @URL[t.co]\n",
      "https://t.co/cvaqwojj4a --- @URL[t.co]\n",
      "https://t.co/pnulv4cx1y --- @URL[t.co]\n",
      "https://t.co/dzcyihtjdo --- @URL[t.co]\n",
      "https://t.co/clkemg4vde --- @URL[t.co]\n",
      "https://t.co/pp9niuh7ru --- @URL[t.co]\n",
      "https://t.co/irk93bqjus --- @URL[t.co]\n",
      "https://t.co/v6md5qubwx --- @URL[t.co]\n",
      "https://t.co/jr67ublot7 --- @URL[t.co]\n",
      "https://t.co/kmkjb5odno --- @URL[t.co]\n",
      "########## Step - Convert urls part 1.5:\n",
      "Unknown words: 39203 | Known words: 7949\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it))\n",
    "def remove_links(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "    temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "\n",
    "    for word in temp_dict:\n",
    "        new_value = temp_dict[word]\n",
    "        if word.find('http')>2:\n",
    "            temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value, URL_TAG)\n",
    "        else:\n",
    "            temp_dict[word] = place_hold(new_value, URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "\n",
    "    # Remove twitter urls\n",
    "    temp_dict = {\n",
    "        f'{URL_TAG}[t.co]': ''\n",
    "    }\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1.5:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove escaped html:\n",
      "Unknown words: 39129 | Known words: 7951\n",
      "term-&gt; --- term-\n",
      "&gt;&gt;&gt;&gt;#bitcoinz&lt;&lt;&lt;&lt; --- #bitcoinz\n",
      "p&amp;l. --- p and l.\n",
      "h&amp;s? --- h and s?\n",
      "&gt;= --- =\n",
      "faang&amp;m --- faang and m\n",
      "&gt;^^&lt; --- ^^\n",
      "s&amp;p --- s and p\n",
      "c&amp;h --- c and h\n",
      "(&gt;10x). --- (10x).\n"
     ]
    }
   ],
   "source": [
    "# Remove escaped html\n",
    "def remove_escaped_html(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    symbols = {\n",
    "        '&quot;': '',\n",
    "        '&amp;': ' and ',\n",
    "        '&lt;': '',\n",
    "        '&gt;': '',\n",
    "    }\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if any([rep in word for rep in symbols.keys()]):\n",
    "            new_word = word\n",
    "            for rep, to in symbols.items():\n",
    "                new_word = new_word.replace(rep, to)\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove escaped html:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_escaped_html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 39129 | Known words: 7951\n",
      "www.maverick-tech.con --- @URL[maverick-tech.con]\n",
      ".www.rapidsnetwork.io --- @URL[rapidsnetwork.io]\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "def convert_urls2(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        url_check = False\n",
    "        if 'file:' in word:\n",
    "            url_check = True\n",
    "        elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "            if 'Aww' not in word:\n",
    "                for d_zone in url_extensions:\n",
    "                    if '.' + d_zone in word:\n",
    "                        url_check = True\n",
    "                        break\n",
    "        elif ('/' in word) and ('.' in word):\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone + '/' in word:\n",
    "                    url_check = True\n",
    "                    break\n",
    "\n",
    "        if url_check:\n",
    "            temp_dict[word] =  place_hold(domain_search(word), URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_urls2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 39128 | Known words: 7951\n",
      ":-)! --- 😁!\n",
      ":-) --- 😁\n",
      ":))) --- 😁)\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if (pict in word) and (len(pict)>2):\n",
    "                    char_pict = pict[-1].isalpha() and pict[0].isalpha()\n",
    "                    if char_pict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "                elif pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 36781 | Known words: 7974\n",
      "🥮👹▶🍞💕🥈🍻🤞😖🦗⛈😧🐈💹🛍😘🤯🔺🎆😊📢🙄🛫🥑📐🚒😬🔪👭🎇🛰🥁♦💥💓👆🆚🎍❓📯☄🟧🌕🤑🌲🆗🚑🖖🦆🤧😒🎱⚽🤳▪😞🎁☝🏈📦💜▫🔎🐐🍄😝🍊📍☕👂😛👥💨🍀👬💇➖😴🌓🐮😌😯🤡🫂🖤💲🌘⚪😥⏲🎨👾🔮💧🔆🏴🎢🦡🦺🏂🏀🅰🐒🧯🐎🤏✔🔝🧄🏠🦄🐃🐂🍸🌧⏫📲✅⤴🌍🕯⁉👷☢😑🍒🌸🙁💻🟨😩⚫🎞📱💊🍌🎵🤓💛💤🚘🦢🌝🙋💙🤴🌛⚡🔫⛔🆙💱💟🦎🗣🌼⚒🧁🟩✈📡✋™😲🧵🤸💁🏭👈🌀🥵📣🤖🌹😋➕♉🏿⬜🚦📉🏇🔼⏰📺🪙🚫⏬🔐🙃📆🦮💸🔟🚀💣⬆🐷😆💬🥺🔶🐶👺👌😮💦💌♀🌒👨🔁😳♣🤦🛀❄😭🍡🌚🔒🔄⛪🙇😪⌚🕷🤬🧘🧠💴🥲📗🐙🤩📝🆒💡🥒👐🤐🪖🍦🎦🤍🏵🔑🔴🥱👟💉‼✳🪦🔹🙀🚆😰⛏📹🎈🔽🙅🟠🧢😔🌏🦉🤘🦁🐦😎🪅👁🔗🃏⚠💎🤗💩😤🖐🛒👣🧿😀✊🤌🏯🌠🔯🔔❣🍼🤲🏦🤷🏽〰🎄⬛📊🔊🐰↗✨☑⛳😷🎲⛓🙏🚩🦅🎉🚗🔵❕🚶🖌😉🩸🎩♥🦈🏾🧧🧑🥀⚙☺🛠🐾®🎥🥶🙆😨💶🏗🛸〽😇❇🏄👽☮🦞🤠🤢🌴🔃🥅🧡🐲💋🗻🤨🥓🌈🏃😼🥴👎🥕⭐♎🔖🔸🚄😄🦾💰🌐👻🐼♾🙂🦕🐺💚⏳👄😐👀😜🖼🎯🥰⛅🥜🎧🟥↩©🌱🛤🤤🌿☹🎟😈👏🏖🌙🍺⏯🟢🐄💼🍳🐋✌🧙🍔❤🔂💯🔱⚛👇🤭🥇🥳😻🤔💃🍓🥃🚊⚜⤵👕🍕🧪🔘🍎⛷♂🍿😠😓🐻🏼⛽❌💞🎭ℹ❗🐳❔🏅🏫📅⬅😙☁🤫🅱🛡📖🚂🥂🎶🍹💫🧚😁😱😍🔥👍🏆💷🥉♻👉⛵🌃🛑😢🦽🚨🎮🎊🙌👸🌻🌊✍😶🍫🦚💠🧨🌑🍾🤚📸🐵👋🦖🚋🥩⚔☎⌛🙊🕊🎓🤟🏻⏱🥥🗳🗑☠🌪📈🎣↪🌋🪐😚💔🥛🥸📌🐕🐸🧷🔷😟🤜😏💘🎤🦬🗨🧸➡💵🕺📚👩🔨🐣😃🚚🔻🧐🙈🌌📞🥊🐬🐟🥬⛴🐢🕘⬇🥞🖕😡🍩🍮📩🎬🌖🔋🌔😂🕵😅🌇🌞🤪🤣🦋🛎🔀🦵📰🦧🦍🦑🌟🦊🔛🏧💀🤝🏹🔌🤙🐍🌜💗🎖🤛◽👤🏁👑💳☀🌎💪🕶🍷🐝👊🔜🏡🎰💭🍏💖😫🌗🗓🏋😵🐑🚣🗽🖇🌳👶\n"
     ]
    }
   ],
   "source": [
    "def isolate_emoji(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if c in emoji_dict])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 563,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 34752 | Known words: 8028\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "def deduplicate_dots(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "            if (Counter(word)['.']>1):\n",
    "                new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "            if (Counter(word)['!']>1):\n",
    "                new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "            if (Counter(word)['?']>1):\n",
    "                new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "            if (Counter(word)[',']>1):\n",
    "                new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(deduplicate_dots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 34738 | Known words: 8028\n",
      "________________________ --- \n",
      "________ --- \n",
      "_____________________ --- \n",
      "__________ --- \n",
      "___ --- \n",
      "#____ --- #\n",
      "______ --- \n",
      "_____________ --- \n",
      "#___ --- #\n",
      "\\_()_/ --- \\()/\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "def remove_underscore_spam(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "            temp_dict[word] = re.sub('_', '', word)\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_underscore_spam)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 34729 | Known words: 8028\n",
      "***** ---  * \n",
      ")))) ---  ) \n",
      "*** ---  * \n",
      "$$$ ---  $ \n",
      "$$$$ ---  $ \n",
      "::::::::::::::::::::::::::: ---  : \n",
      "$$$$$$$$$$$$ ---  $ \n",
      "$$$$$ ---  $ \n",
      "**** ---  * \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "def isolate_spam_characters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "            temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(1)])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_spam_characters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 566,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 34724 | Known words: 8028\n",
      "=) --- 😁\n",
      ";) --- 😜\n",
      ":( --- 😡\n",
      ":] --- 😁\n",
      ":) --- 😁\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 567,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 33135 | Known words: 8087\n",
      "40 ---  ( \n",
      "41 ---  ) \n",
      "91 ---  [ \n",
      "93 ---  ] \n",
      "123 ---  { \n",
      "125 ---  } \n",
      "60 ---  < \n",
      "62 ---  > \n",
      "34 ---  \" \n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "def isolate_brackets(texts):\n",
    "    chars = '()[]{}<>\"'\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_brackets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 568,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Extract date and time:\n",
      "Unknown words: 33137 | Known words: 8087\n",
      "14:26:27 ---  @TIME[14:26:27] \n",
      "12/30/20 ---  @DATE[12/30/20] \n",
      "16:17:20 ---  @TIME[16:17:20] \n",
      "2/6/21 ---  @DATE[2/6/21] \n",
      "07/02/2021 ---  @DATE[07/02/2021] \n",
      "15:00:02: ---  @TIME[15:00:02] :\n",
      "5/22/10, ---  @DATE[5/22/10] \n",
      "14:00:02: ---  @TIME[14:00:02] :\n",
      "2/2/21 ---  @DATE[2/2/21] \n",
      "18:00:02: ---  @TIME[18:00:02] :\n"
     ]
    }
   ],
   "source": [
    "# Extract date and time\n",
    "def extract_date_and_time(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_fix = re.compile('^[$£%€][-+][0-9]')\n",
    "    time_regex = re.compile('([0-9]{1,2}:[0-9]{1,2}:[0-9]{1,4})')\n",
    "    date_regex = re.compile('([0-9]{1,4}\\/[0-9]{1,2}\\/[0-9]{1,4})')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "\n",
    "        ## -------- Time\n",
    "        time_result = time_regex.search(prefilter)\n",
    "        if time_result:\n",
    "            prefix = prefilter[:time_result.start()]\n",
    "            suffix = prefilter[time_result.end():]\n",
    "            mpart = prefilter[time_result.start():time_result.end()]\n",
    "            temp_dict[word] = ' '.join([\n",
    "                prefix,\n",
    "                place_hold(str(mpart), TIME_TAG),\n",
    "                suffix\n",
    "            ])\n",
    "            continue\n",
    "\n",
    "        ## -------- Date\n",
    "        date_result = date_regex.search(prefilter.replace('-', '/'))\n",
    "        if date_result and len(word.split('/')) == 3:\n",
    "            prefix = prefilter[:date_result.start()]\n",
    "            suffix = prefilter[date_result.end():]\n",
    "            mpart = prefilter[date_result.start():date_result.end()]\n",
    "            temp_dict[word] = ' '.join([\n",
    "                prefix,\n",
    "                place_hold(str(mpart), DATE_TAG),\n",
    "                suffix\n",
    "            ])\n",
    "            continue\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Extract date and time:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_date_and_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 569,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 33132 | Known words: 8087\n",
      "chg --- change\n",
      "b4 --- before\n",
      "u.s. --- united states\n",
      "m.cap --- market cap\n",
      "mkt --- market\n"
     ]
    }
   ],
   "source": [
    "def custom_global_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_general_synonyms:\n",
    "            temp_dict[word] = helper_custom_general_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom global word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_global_synonyms)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 570,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break short words:\n",
      "Unknown words: 32784 | Known words: 8104\n",
      "2/2 --- 2 / 2\n",
      "$50/share --- $50 / share\n",
      "#eos/#btc --- #eos / #btc\n",
      "#kyc/#aml --- #kyc / #aml\n",
      "usd/btc, --- usd / btc,\n",
      "$tfuel/ --- $tfuel / \n",
      "$celr/ --- $celr / \n",
      "#ada/#btc --- #ada / #btc\n",
      "9/10 --- 9 / 10\n",
      "24/7! --- 24 / 7!\n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "def break_short_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break short words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(break_short_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 571,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 32786 | Known words: 8108\n",
      "hurdle-turned-support --- hurdle turned support\n",
      "revenue/terahash-second --- revenue / terahash-second\n",
      "/jonathan/gabriel/ozo ---  / jonathan / gabriel / ozo\n",
      "casino-partner/stakeholder. --- casino-partner/stakeholder . \n",
      "#fashion.#beautiful.#happy.#cute. --- #fashion . #beautiful . #happy . #cute . \n",
      "espadora@protonmail.com --- espadora@protonmail . com\n",
      "caaaaanntaaaareeee.oh.oh.oh.oh --- caaaaanntaaaareeee . oh . oh . oh . oh\n",
      "crypto-dinner-futures --- crypto dinner futures\n",
      "instagram@abiolaa.apparel --- instagram@abiolaa . apparel\n",
      "every-once-in-a-while, --- every-once-in-a-while , \n",
      "########## Step - Break long words:\n",
      "Unknown words: 32784 | Known words: 8108\n",
      "casino-partner/stakeholder --- casino-partner / stakeholder\n",
      "pullback/consolidation --- pullback / consolidation\n",
      "august/september/october --- august / september / october\n",
      "every-once-in-a-while --- every once in a while\n",
      "########## Step - Break long words:\n",
      "Unknown words: 32784 | Known words: 8108\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "def break_long_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '_' in word and not (len(word) > 2 and word[0] in ['#', '$', '@'] and word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            temp_dict[word] = re.sub('_', ' ', word)\n",
    "        elif '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "        elif len(' '.join(word.split('-')).split())>2:\n",
    "            temp_dict[word] = re.sub('-', ' ', word)\n",
    "        for s in ',.:;':\n",
    "            if s in word and not re.compile('[+#@$/,.:;-]').sub('', word).isnumeric():\n",
    "                temp_dict[word] = word.replace(s, f' {s} ')\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "for i in range(3):\n",
    "    texts = texts.pipe(break_long_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 32674 | Known words: 8109\n",
      "200$s --- 200 $s\n",
      "=#algo --- = #algo\n",
      "volatility?#crypto --- volatility? #crypto\n",
      "nigeria.#bitcoin --- nigeria. #bitcoin\n",
      "~$760 --- ~ $760\n",
      "-$dash --- - $dash\n",
      "gold.#dent --- gold. #dent\n",
      "dm@or --- dm @or\n",
      "me.#bitcoin --- me. #bitcoin\n",
      "-@peterschiff --- - @peterschiff\n"
     ]
    }
   ],
   "source": [
    "# TODO: add number parsing before\n",
    "# Diambiguate entities\n",
    "# Split words on @,# and $ to clear up ambiguities between entitites\n",
    "def disambiguate_entitites(texts):\n",
    "    symbols = '@#$'\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('@' in k or '#' in k or '$' in k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        for symbol in symbols:\n",
    "            if symbol not in word: continue\n",
    "            left, *right = word.split(symbol)\n",
    "            rightz = symbol.join(right)\n",
    "            if len(left) > 0 and len(right[0]) > 0 and right[0].isalnum():\n",
    "                temp_dict[word] = f'{left} {symbol}{rightz}'\n",
    "            break\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Disambiguate entities:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(disambiguate_entitites)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 573,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 32637 | Known words: 8109\n",
      "bitmex --- @bitmex\n",
      "crypto --- #cryptocurrency\n",
      "binance --- @binance\n",
      "bitstamp --- @bitstamp\n",
      "cointelegraph --- @cointelegraph\n",
      "hodl --- #hodl\n",
      "@crypto --- #cryptocurrency\n",
      "poloniex --- @poloniex\n",
      "$crypto --- #cryptocurrency\n",
      "#bitpay --- @bitpay\n"
     ]
    }
   ],
   "source": [
    "def custom_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_synonyms:\n",
    "            temp_dict[word] = helper_custom_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 32361 | Known words: 8109\n",
      "$cos --- $contentos\n",
      "$bscx --- $bscex\n",
      "$xfi --- $dfinance\n",
      "@nexo --- $nexo\n",
      "$ast --- $antiscamtoken\n",
      "btt --- $bittorrent\n",
      "$daiq --- $daiquilibrium\n",
      "hbar --- $hedera_hashgraph\n",
      "$mta --- $meta\n",
      "#bancor --- $bancor\n"
     ]
    }
   ],
   "source": [
    "def custom_currency_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_currency_synonyms:\n",
    "            temp_dict[word] = helper_currency_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom currency synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_currency_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 31629 | Known words: 8109\n",
      "@googlepay --- @USR[googlepay]\n",
      "@joesurebangers1 --- @USR[joesurebangers1]\n",
      "$rarible --- @HTAG[rarible]\n",
      "#reddcoin --- @HTAG[reddcoin]\n",
      "#bitvavo --- @HTAG[bitvavo]\n",
      "#ceo --- @HTAG[ceo]\n",
      "#zom --- @HTAG[zom]\n",
      "#payment, --- @HTAG[payment]\n",
      "$shop --- @HTAG[shop]\n",
      "#wallets --- @HTAG[wallets]\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags\n",
    "def extract_entities(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(word) > 2) and (word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            new_word = word.replace('\\'s', '')\n",
    "            if not re.compile('[#@$/,.:;]').sub('', new_word).isnumeric():\n",
    "                new_word = re.compile('[,.:;]').sub('', new_word)\n",
    "                if word.startswith('@'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], USER_TAG)\n",
    "                elif word.startswith('#'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], HASH_TAG)\n",
    "                elif word.startswith('u/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], USER_TAG)\n",
    "                elif word.startswith('r/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], HASH_TAG)\n",
    "                elif word.startswith('$') and new_word[1:].replace('_', '').isalpha():\n",
    "                    tag = CURRENCY_TAG if word[1:] in helper_currency_synonyms else HASH_TAG\n",
    "                    temp_dict[word] = place_hold(new_word[1:], tag)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 31595 | Known words: 8109\n",
      "@HTAG[iota] --- @CURR[iota]\n",
      "@USR[algorand] --- @CURR[algorand]\n",
      "@HTAG[omg] --- @CURR[omg]\n",
      "@HTAG[aave] --- @CURR[aave]\n",
      "@HTAG[ethereum] --- @CURR[ethereum]\n",
      "@USR[ethereum] --- @CURR[ethereum]\n",
      "@HTAG[pancakeswap] --- @CURR[pancakeswap]\n",
      "@USR[pancakeswap] --- @CURR[pancakeswap]\n",
      "@HTAG[bitcoin] --- @CURR[bitcoin]\n",
      "@USR[bitcoin] --- @CURR[bitcoin]\n"
     ]
    }
   ],
   "source": [
    "# Hashtag and currency union\n",
    "def hashtag_currency_union(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = set([k for k in temp_vocab if not check_replace(k)])\n",
    "    temp_dict = {}\n",
    "    for w in temp_vocab:\n",
    "        if w.startswith(CURRENCY_TAG):\n",
    "            if w.replace(CURRENCY_TAG, HASH_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, HASH_TAG)] = w\n",
    "            if w.replace(CURRENCY_TAG, USER_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, USER_TAG)] = w\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Hashtag and currency union:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 31595 | Known words: 8109\n",
      "usdt_ --- usdt\n",
      "'fu__ --- 'fu\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "def remove_ending_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[len(word)-1]=='_':\n",
    "            for i in range(len(word),0,-1):\n",
    "                if word[i-1]!='_':\n",
    "                    new_word = word[:i]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_ending_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 31595 | Known words: 8109\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore\n",
    "def remove_starting_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[0]=='_':\n",
    "            for i in range(len(word)):\n",
    "                if word[i]!='_':\n",
    "                    new_word = word[i:]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_starting_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 579,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 23577 | Known words: 8585\n",
      "thousandaire. --- thousandaire .\n",
      "wedge, --- wedge ,\n",
      "breakfast. --- breakfast .\n",
      "weapon! --- weapon !\n",
      "ever, --- ever ,\n",
      "payment! --- payment !\n",
      "wallet! --- wallet !\n",
      "2019. --- 2019 .\n",
      "10months, --- 10months ,\n",
      "refarens: --- refarens :\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "def end_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1].isnumeric() and re.compile('[$£%€]').match(word[i]):\n",
    "                break\n",
    "\n",
    "            if word[i-1].isalnum():\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(end_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 580,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21760 | Known words: 8605\n",
      "$24,100,500 --- @NUM[24100500.0] usd\n",
      "$38333 --- @NUM[38333.0] usd\n",
      "5.79 --- @NUM[5.79]\n",
      "585$ --- @NUM[585.0] usd\n",
      "1,329% --- @NUM[1329.0] percent\n",
      "5%, --- @NUM[5.0] percent\n",
      "42.000 --- @NUM[42000.0]\n",
      "2140 --- @NUM[2140.0]\n",
      ".002,time --- @NUM[2.0] time\n",
      "$39500 --- @NUM[39500.0] usd\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21569 | Known words: 8605\n",
      "35xxx --- @NUM[35.0] xxx\n",
      "$12.50 --- @NUM[12.5] usd\n",
      "3.76 --- @NUM[3.76]\n",
      "30%. --- @NUM[30.0] percent .\n",
      "45000 --- @NUM[45000.0]\n",
      "140% --- @NUM[140.0] percent\n",
      "$1000 --- @NUM[1000.0] usd\n",
      "$80k --- @NUM[80000.0] usd\n",
      "$85k --- @NUM[85000.0] usd\n",
      "40000 --- @NUM[40000.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21567 | Known words: 8605\n",
      "300$5001000$2000 --- @NUM[300.0] usd 5001000$2000\n",
      "78$ --- @NUM[78.0] usd\n",
      "^24 --- @NUM[24.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21567 | Known words: 8605\n",
      "5001000$2000 --- @NUM[5001000.0] usd 2000\n"
     ]
    }
   ],
   "source": [
    "scale_mapping = {\n",
    "    'b': 1000000000,\n",
    "    'bn': 1000000000,\n",
    "    'bln': 1000000000,\n",
    "    'billion': 1000000000,\n",
    "    'm': 1000000,\n",
    "    'mn': 1000000,\n",
    "    'mln': 1000000,\n",
    "    'million': 1000000,\n",
    "    'k': 1000,\n",
    "    'thousand': 1000,\n",
    "    '-': -1,\n",
    "}\n",
    "\n",
    "translate = {\n",
    "    '$': 'usd', '£': 'gbp','%': 'percent', '€': 'eur'\n",
    "}\n",
    "\n",
    "translate_suffix = {\n",
    "    'x': 'times'\n",
    "}\n",
    "\n",
    "translate_prefix = {\n",
    "    '~': 'around',\n",
    "    '+-': 'around',\n",
    "    '±': 'around',\n",
    "    '@': 'at',\n",
    "    '=': 'equals',\n",
    "    '*#': 'ranked',\n",
    "    '#': 'ranked',\n",
    "}\n",
    "\n",
    "def serialize_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_num = re.compile('^(~|\\+-|±|@|=|#|\\*#)?[-@+*^#:]?[$£%€]?(([.:]?[0-9])+)[$£%€]?')\n",
    "    re_fix = re.compile('^[$£%€][-+][0-9]')\n",
    "    time_regex = re.compile('([0-9]{1,2}:[0-9]{1,2}:[0-9]{1,4})')\n",
    "    date_regex = re.compile('([0-9]{1,4}\\/[0-9]{1,2}\\/[0-9]{1,4})')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "\n",
    "        ## ----- Various other numbers\n",
    "        result = re_num.search(prefilter)\n",
    "        if result and result.pos == 0:\n",
    "            # Process combined numbers / ranges in next iteration\n",
    "            if '-' in word and not word.startswith('-') and not word.startswith('+-'):\n",
    "                temp_dict[word] = ' '.join(word.split('-'))\n",
    "                continue\n",
    "\n",
    "            main_part = prefilter[:result.end()]\n",
    "            prefix = ''\n",
    "            for prefix_key, prefix_name in translate_prefix.items():\n",
    "                if main_part.startswith(prefix_key):\n",
    "                    prefix = prefix_name\n",
    "                    main_part = main_part.replace(prefix_key, '', 1)\n",
    "                    break\n",
    "\n",
    "            main = re.compile('^[~@+*^#:]').sub('',main_part)\n",
    "            currency = re.compile('[$£%€]').search(main)\n",
    "            currency = main[currency.start():currency.end()] if currency else None\n",
    "            main = re.compile('[$£%€]').sub('', main)\n",
    "            suffix = prefilter[result.end():]\n",
    "\n",
    "            multiplier = 1\n",
    "            if re.compile('\\.[0-9]{1,2}$').search(main): # decimal\n",
    "                multiplier *= 0.01 if main[-1].isnumeric() else 0.1\n",
    "            if '-' in main: # Neg numbers\n",
    "                multiplier *= -1\n",
    "                main = main.replace('-', '')\n",
    "            # Textual scale\n",
    "            if suffix in scale_mapping:\n",
    "                multiplier *= scale_mapping[suffix]\n",
    "                suffix = ''\n",
    "            if suffix in translate_suffix:\n",
    "                suffix = translate_suffix[suffix]\n",
    "\n",
    "            number = round(float(main.replace('.', '').replace(':', '')) * multiplier, 2)\n",
    "            # print(f'{number}  /  {currency}  /  {suffix}  /  {word}')\n",
    "            # noinspection PyTypeChecker\n",
    "            temp_dict[word] = ' '.join(filter(len,[\n",
    "                prefix,\n",
    "                place_hold(str(number), NUMBER_TAG),\n",
    "                translate[currency] if currency else '',\n",
    "                suffix\n",
    "            ]))\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Serialize numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Clean up numbers\n",
    "for i in range(4):\n",
    "    texts = texts.pipe(serialize_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 581,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 21564 | Known words: 8605\n",
      "b4 --- before\n",
      "sh#t --- shit\n",
      "m.cap --- market cap\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 21567 | Known words: 8605\n",
      "'#eloneffect --- ' #eloneffect\n",
      "target-$700 --- target- $700\n",
      "here:@kucoincom --- here: @kucoincom\n",
      ".@nasdaq --- . @nasdaq\n",
      "target-$630 --- target- $630\n",
      "'#finance --- ' #finance\n",
      "video,@elliotrades --- video, @elliotrades\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 21563 | Known words: 8605\n",
      "crypto --- #cryptocurrency\n",
      "binance --- @binance\n",
      "cointelegraph --- @cointelegraph\n",
      "hodl --- #hodl\n",
      "poloniex --- @poloniex\n",
      "cryptocurrencies --- #cryptocurrency\n",
      "airdrop --- #airdrop\n",
      "bitmain --- @bitmain\n",
      "kraken --- @kraken\n",
      "blockchain --- #blockchain\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 21541 | Known words: 8605\n",
      "hbar --- $hedera_hashgraph\n",
      "cny --- $cny\n",
      "tether --- $tether\n",
      "sushi --- $sushiswap\n",
      "$ont --- $ontology\n",
      "trx --- $tron\n",
      "xrp --- $xrp\n",
      "$doge --- $dogecoin\n",
      "$sushi --- $sushiswap\n",
      "cel --- $celsius\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 21452 | Known words: 8605\n",
      "$sushiswap --- @CURR[sushiswap]\n",
      "$aave --- @CURR[aave]\n",
      "$blocknet --- @HTAG[blocknet]\n",
      "#hodl --- @HTAG[hodl]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "$decentraland --- @CURR[decentraland]\n",
      "$curve_dao_token --- @CURR[curve_dao_token]\n",
      "#airdrop --- @HTAG[airdrop]\n",
      "$bitcoin_sv --- @CURR[bitcoin_sv]\n",
      "$aud --- @CURR[aud]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 21452 | Known words: 8605\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 582,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 21178 | Known words: 8612\n",
      "*police --- * police\n",
      "'rare --- ' rare\n",
      "*before --- * before\n",
      "'qui --- ' qui\n",
      "*eden --- * eden\n",
      "*enter --- * enter\n",
      "'it's --- ' it's\n",
      "'fu --- ' fu\n",
      "-company --- - company\n",
      "*ahem --- * ahem\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "def start_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum() and k[0] not in ['@', '#', '$'])]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word)):\n",
    "            if word[i].isalnum() or word[i] in ['#', '@', '$']:\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(start_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 583,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 21178 | Known words: 8612\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 21178 | Known words: 8612\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 21173 | Known words: 8612\n",
      "4241491.0 --- @NUM[424149.1]\n",
      "250m --- @NUM[250000000.0]\n",
      "6.11 --- @NUM[6.11]\n",
      "$0x --- @NUM[0.0] usd times\n",
      "4301056.0 --- @NUM[430105.6]\n",
      "5058389.0 --- @NUM[505838.9]\n",
      "4233436.0 --- @NUM[423343.6]\n",
      "$700 --- @NUM[700.0] usd\n",
      "100% --- @NUM[100.0] percent\n",
      "4240291.0 --- @NUM[424029.1]\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 21172 | Known words: 8612\n",
      "crypto --- #cryptocurrency\n",
      "cryptocurrency --- #cryptocurrency\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 21170 | Known words: 8612\n",
      "dogecoin --- $dogecoin\n",
      "doge --- $dogecoin\n",
      "bitcoin --- $bitcoin\n",
      "ethereum --- $ethereum\n",
      "eth --- $ethereum\n",
      "matic --- $polygon\n",
      "zil --- $zilliqa\n",
      "bnb --- $binance_coin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 21162 | Known words: 8612\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "@swagbag_stores --- @USR[swagbag_stores]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "$polygon --- @CURR[polygon]\n",
      "@monster_soundz --- @USR[monster_soundz]\n",
      "#cryptocurrency --- @HTAG[cryptocurrency]\n",
      "$zilliqa --- @CURR[zilliqa]\n",
      "@michael_saylor --- @USR[michael_saylor]\n",
      "$dogecoin --- @CURR[dogecoin]\n",
      "$binance_coin --- @CURR[binance_coin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 21162 | Known words: 8612\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 584,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 21162 | Known words: 8612\n",
      "p.o.d --- word_placeholder[pod]\n",
      "f.i.a.t --- word_placeholder[fiat]\n",
      "g.o.a.t --- word_placeholder[goat]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "def find_replace_acronyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "            if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "                temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "            else:\n",
    "                if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                    temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(find_replace_acronyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 585,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Contractions:\n",
      "Unknown words: 21100 | Known words: 8612\n",
      "we're --- we are\n",
      "you'd --- you would\n",
      "we'll --- we will\n",
      "there'll --- there will\n",
      "here's --- here is\n",
      "ya'll --- you will\n",
      "would've --- would have\n",
      "she's --- she is\n",
      "when's --- when is\n",
      "this's --- this is\n"
     ]
    }
   ],
   "source": [
    "# Apply spellchecker for contractions\n",
    "def apply_spellchecker_contractions(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_contractions:\n",
    "            temp_dict[word] = helper_contractions[word] # place_hold(helper_contractions[word])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(apply_spellchecker_contractions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 586,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove \"s:\n",
      "Unknown words: 20889 | Known words: 8622\n",
      "#ether's --- #ether\n",
      "night's --- night\n",
      "god's --- god\n",
      "kc's --- kc\n",
      "buhari's --- buhari\n",
      "etf's --- etf\n",
      "#derivatives-what's --- #derivatives-what\n",
      "@paypal's --- @paypal\n",
      "father's --- father\n",
      "who''s --- who'\n"
     ]
    }
   ],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "def remove_comma_s(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_comma_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 20889 | Known words: 8622\n"
     ]
    }
   ],
   "source": [
    "def convert_backslash(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]\n",
    "    temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_backslash)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 20889 | Known words: 8622\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 20889 | Known words: 8622\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 20889 | Known words: 8622\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 20884 | Known words: 8622\n",
      "crypto --- #cryptocurrency\n",
      "binance --- @binance\n",
      "#crypto --- #cryptocurrency\n",
      "blockchain --- #blockchain\n",
      "paypal --- @paypal\n",
      "cryptocurrency --- #cryptocurrency\n",
      "#binance --- @binance\n",
      "coinbase --- @coinbase\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 20873 | Known words: 8622\n",
      "@dogecoin --- $dogecoin\n",
      "cardano --- $cardano\n",
      "iota --- $iota\n",
      "#cardano --- $cardano\n",
      "@cardano --- $cardano\n",
      "#iota --- $iota\n",
      "dogecoin --- $dogecoin\n",
      "doge --- $dogecoin\n",
      "chainlink --- $chainlink\n",
      "bitcoin --- $bitcoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 20823 | Known words: 8622\n",
      "#schrammel --- @HTAG[schrammel]\n",
      "$aave --- @CURR[aave]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "@riodefiofficial --- @USR[riodefiofficial]\n",
      "#ether --- @HTAG[ether]\n",
      "@humblpay --- @USR[humblpay]\n",
      "@sylo --- @USR[sylo]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "@coinbase --- @USR[coinbase]\n",
      "#etheruem --- @HTAG[etheruem]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 20823 | Known words: 8622\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 589,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Dup chars (with vocab check):\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "lettttssss --- lets\n",
      "pappa --- papa\n",
      "buyyy --- buy\n",
      "holdddddd --- hold\n",
      "ooooof --- of\n",
      "reccent --- recent\n",
      "seee --- se\n",
      "doon --- don\n",
      "nahhhhh --- nah\n",
      "shittttt --- shit\n"
     ]
    }
   ],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!). TODO check fist against vocab?\n",
    "def remove_duplicated_character(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_vocab_dup = []\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "    temp_vocab_dup = set(temp_vocab_dup)\n",
    "    temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "        if new_word in temp_vocab_dup:\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_duplicated_character)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 20573 | Known words: 8657\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 20573 | Known words: 8657\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 591,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate numbers:\n",
      "Unknown words: 20573 | Known words: 8657\n"
     ]
    }
   ],
   "source": [
    "def isolate_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "            if re.compile('[0-9]').sub('', word) != word:\n",
    "                temp_dict[word] = word\n",
    "\n",
    "    global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    temp_dict = {k:place_hold(k) for k in temp_dict}\n",
    "\n",
    "    #texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 20567 | Known words: 8657\n",
      "-- --- -\n",
      "--- --- -\n",
      "#crypto!--where --- #crypto!-where\n",
      "clockwork--up --- clockwork-up\n",
      "------------------------------------------ --- -\n",
      "------------- --- -\n",
      "---- --- -\n",
      "----- --- -\n",
      "aa--tag --- aa-tag\n",
      "outshined--cryptocurrency --- outshined-cryptocurrency\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "def join_dashes(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_dashes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 20567 | Known words: 8657\n"
     ]
    }
   ],
   "source": [
    "# Try join word (Sloooow)\n",
    "def join_word_letters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "        if (new_word in local_vocab) and (len(new_word)>3):\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_word_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 594,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 19703 | Known words: 8746\n",
      "💓 ---  💓 \n",
      "a:$39,302 --- a :  $ 39 , 302\n",
      "yayında --- yay ı nda\n",
      "🚆 ---  🚆 \n",
      "#crypto.com ---  # crypto . com\n",
      "re-test --- re - test\n",
      "this,please --- this , please\n",
      "meta-ratio --- meta - ratio\n",
      "alarm_clock --- alarm _ clock\n",
      ",- ---  ,  - \n"
     ]
    }
   ],
   "source": [
    "# TODO: _ should become ' ' and we should preserve numbers or hashtags\n",
    "# Try Split word\n",
    "def split_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "            chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "            temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(split_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 19699 | Known words: 8749\n",
      "fa1 --- fai\n",
      "t13 --- tie\n",
      "sh1t --- shit\n",
      "or3 --- ore\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion\n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "\n",
    "def convert_leet_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = convert_leet(word)\n",
    "        if (new_word!=word):\n",
    "            if (len(word)>2) and (new_word in local_vocab):\n",
    "                temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_leet_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 19699 | Known words: 8749\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 19679 | Known words: 8750\n",
      "18033 --- @NUM[18033.0]\n",
      "047 --- @NUM[47.0]\n",
      "074 --- @NUM[74.0]\n",
      "00063229 --- @NUM[63229.0]\n",
      "011360 --- @NUM[11360.0]\n",
      "8657 --- @NUM[8657.0]\n",
      "931 --- @NUM[931.0]\n",
      "50hz --- @NUM[50.0] hz\n",
      "1393 --- @NUM[1393.0]\n",
      "012757 --- @NUM[12757.0]\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 19678 | Known words: 8750\n",
      "crypto --- #cryptocurrency\n",
      "binance --- @binance\n",
      "bitstamp --- @bitstamp\n",
      "hodl --- #hodl\n",
      "blockchain --- #blockchain\n",
      "altcoins --- #altcoins\n",
      "bitcoins --- $bitcoin\n",
      "cryptocurrency --- #cryptocurrency\n",
      "coinbase --- @coinbase\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 19673 | Known words: 8750\n",
      "trx --- $tron\n",
      "xrp --- $xrp\n",
      "yfi --- $yearn_finance\n",
      "usdt --- $tether\n",
      "polkadot --- $polkadot_new\n",
      "avax --- $avalanche\n",
      "algorand --- $algorand\n",
      "egld --- $elrond_egld\n",
      "dogecoin --- $dogecoin\n",
      "doge --- $dogecoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 19643 | Known words: 8750\n",
      "$aave --- @CURR[aave]\n",
      "#hodl --- @HTAG[hodl]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "$aud --- @CURR[aud]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "$maker --- @HTAG[maker]\n",
      "@coinbase --- @USR[coinbase]\n",
      "$litecoin --- @CURR[litecoin]\n",
      "$ontology --- @CURR[ontology]\n",
      "$tether --- @CURR[tether]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 19643 | Known words: 8750\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 19640 | Known words: 8752\n"
     ]
    }
   ],
   "source": [
    "# Remove placeholders\n",
    "def remove_placeholders(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (not check_replace(k) and k.startswith(WPLACEHOLDER))]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_placeholders)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 19338 | Known words: 8825\n",
      "somethings --- something\n",
      "panics --- panic\n",
      "shaves --- shave\n",
      "trembles --- tremble\n",
      "nuevos --- nuevo\n",
      "coincides --- coincide\n",
      "merges --- merge\n",
      "eaters --- eater\n",
      "renewables --- renewable\n",
      "consultations --- consultation\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "def search_multiple_form(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "    temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(search_multiple_form)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 19338 | Known words: 8825\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 19338 | Known words: 8825\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 19338 | Known words: 8825\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 19338 | Known words: 8825\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 19338 | Known words: 8825\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 19338 | Known words: 8825\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: 0.0383\n",
      "########## Step - Language datection:\n",
      "Unknown words: 17739 | Known words: 8674\n"
     ]
    }
   ],
   "source": [
    "# Cut away non english tweets\n",
    "model = fasttext.load_model('../../data/kaggle/lid.176.ftz')\n",
    "\n",
    "def langcheck(item, min_confidence=0.2):\n",
    "    text = ' '.join([w for w in item.split() if not w.startswith('@')])\n",
    "    if len(text) < 3:\n",
    "        return True\n",
    "    results = dict(zip(*model.predict(text, k=2)))\n",
    "    return results.get('__label__en', 0) > min_confidence\n",
    "\n",
    "mask = texts.parallel_map(langcheck)\n",
    "if verbose: print(f'Deleted: {1 - sum(mask)/len(texts)}')\n",
    "texts = texts[mask]\n",
    "data = data[mask]\n",
    "if verbose: print('#' * 10, 'Step - Language datection:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "outputs": [
    {
     "data": {
      "text/plain": "                       _id  \\\n0      1360142875330232324   \n1      1360140112861003776   \n2      1360137307047694337   \n4      1360132401142366210   \n5      1360131434158170113   \n...                    ...   \n19995  1357792968455946242   \n19996  1357792933982928896   \n19997  1357792930359107588   \n19998  1357792864005095424   \n19999  1357792837870510083   \n\n                                                                                                                                                                                                          text  \n0      when the top united states central banker gets photobombed by @CURR[bitcoin] . 👉 👀 @CURR[bitcoin] @CURR[bitcoin] @HTAG[cryptocurrency] @HTAG[cryptocurrency] @CURR[ethereum] @CURR[xrp] @CURR[chainl...  \n1      best am arriving with exciting features @HTAG[bsc] @USR[binance] @CURR[bitcoin] @HTAG[binancesmartchain] @HTAG[defi] @HTAG[definews] @HTAG[stafi] @HTAG[cake] @CURR[pancakeswap] @HTAG[paraswap] @HT...  \n2      to keep its ultra bullish run intact , @CURR[elrond_egld] bulls need to keep @CURR[elrond_egld] / @CURR[tether] daily above @NUM[148.0] usd . reclaiming @NUM[174.0] usd would be superb . break @NU...  \n4      next coin that goes @NUM[100.0] percent . . . buckle up . . . @CURR[tezos] @CURR[tezos] @CURR[tezos] look @ my calls from last 2 weeks @CURR[iota] @HTAG[coti] @CURR[tezos] will move hard incoming ...  \n5                        its gonna be huge ! 🚀 😍 👑 @HTAG[fetch_ai] 👑 @CURR[xrp] @CURR[vechain] @CURR[chainlink] @CURR[cardano] @CURR[algorand] @HTAG[altcoins] @HTAG[artificialintelligence] @HTAG[blockchain]  \n...                                                                                                                                                                                                        ...  \n19995                                                                                                                                                                             cash is trash @CURR[bitcoin]  \n19996                                                                                         global central bank efforts to limit united states dollars decline raises specter of currency war @CURR[bitcoin]  \n19997                                                                                                                                       what if @CURR[bitcoin] is a social experiment ? well , money was .  \n19998                                                                                                   @CURR[bitcoin] btw that was pre close ny - cme friday dump . pl are closing positions before weekend .  \n19999                                                                                                                                           nigeria is fucked and pregnant with stupidity . @CURR[bitcoin]  \n\n[19234 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1360142875330232324</td>\n      <td>when the top united states central banker gets photobombed by @CURR[bitcoin] . 👉 👀 @CURR[bitcoin] @CURR[bitcoin] @HTAG[cryptocurrency] @HTAG[cryptocurrency] @CURR[ethereum] @CURR[xrp] @CURR[chainl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1360140112861003776</td>\n      <td>best am arriving with exciting features @HTAG[bsc] @USR[binance] @CURR[bitcoin] @HTAG[binancesmartchain] @HTAG[defi] @HTAG[definews] @HTAG[stafi] @HTAG[cake] @CURR[pancakeswap] @HTAG[paraswap] @HT...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1360137307047694337</td>\n      <td>to keep its ultra bullish run intact , @CURR[elrond_egld] bulls need to keep @CURR[elrond_egld] / @CURR[tether] daily above @NUM[148.0] usd . reclaiming @NUM[174.0] usd would be superb . break @NU...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1360132401142366210</td>\n      <td>next coin that goes @NUM[100.0] percent . . . buckle up . . . @CURR[tezos] @CURR[tezos] @CURR[tezos] look @ my calls from last 2 weeks @CURR[iota] @HTAG[coti] @CURR[tezos] will move hard incoming ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1360131434158170113</td>\n      <td>its gonna be huge ! 🚀 😍 👑 @HTAG[fetch_ai] 👑 @CURR[xrp] @CURR[vechain] @CURR[chainlink] @CURR[cardano] @CURR[algorand] @HTAG[altcoins] @HTAG[artificialintelligence] @HTAG[blockchain]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>1357792968455946242</td>\n      <td>cash is trash @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>1357792933982928896</td>\n      <td>global central bank efforts to limit united states dollars decline raises specter of currency war @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>1357792930359107588</td>\n      <td>what if @CURR[bitcoin] is a social experiment ? well , money was .</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>1357792864005095424</td>\n      <td>@CURR[bitcoin] btw that was pre close ny - cme friday dump . pl are closing positions before weekend .</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>1357792837870510083</td>\n      <td>nigeria is fucked and pregnant with stupidity . @CURR[bitcoin]</td>\n    </tr>\n  </tbody>\n</table>\n<p>19234 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = texts\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO:\n",
    "* numbers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
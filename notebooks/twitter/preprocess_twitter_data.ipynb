{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 16 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    }
   ],
   "source": [
    "# Credit for some parts to: https://www.kaggle.com/kyakovlev/preprocessing-bert-public\n",
    "# Number extraction and hashtags is my baby\n",
    "\n",
    "# General imports|  \n",
    "import pandas as pd\n",
    "import re, warnings, pickle, itertools, emoji, unicodedata\n",
    "\n",
    "# custom imports\n",
    "from gensim.utils import deaccent\n",
    "from collections import Counter\n",
    "from bs4 import BeautifulSoup\n",
    "from utils.datasets import *\n",
    "from pandarallel import pandarallel\n",
    "import fasttext\n",
    "\n",
    "pandarallel.initialize()\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.options.display.max_columns = 10\n",
    "pd.options.display.max_colwidth = 200\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "## Initial vars\n",
    "\n",
    "HELPER_PATH             = '../../data/helpers/'\n",
    "LOCAL_TEST = True       ## Local test - for test performance on part of the train set only\n",
    "verbose = True\n",
    "WPLACEHOLDER = 'word_placeholder'\n",
    "URL_TAG = '@URL'\n",
    "USER_TAG = '@USR'\n",
    "NUMBER_TAG = '@NUM'\n",
    "HASH_TAG = '@HTAG'\n",
    "CURRENCY_TAG = '@CURR'\n",
    "TIME_TAG = '@TIME'\n",
    "DATE_TAG = '@DATE'\n",
    "IMMUTABLES = [\n",
    "    WPLACEHOLDER,\n",
    "    URL_TAG, USER_TAG, NUMBER_TAG, HASH_TAG, CURRENCY_TAG,\n",
    "    TIME_TAG, DATE_TAG\n",
    "]\n",
    "\n",
    "SEED = 42               ## Seed for enviroment\n",
    "seed_everything(SEED)   ## Seed everything"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "## Preprocess helpers\n",
    "def place_hold(w, tag=WPLACEHOLDER):\n",
    "    return tag + '[' + re.sub(' ', '___', w) + ']'\n",
    "\n",
    "## Helpers\n",
    "def check_replace(w):\n",
    "    return not bool(re.search('|'.join(IMMUTABLES), w))\n",
    "\n",
    "def make_cleaning(s, c_dict):\n",
    "    if check_replace(s):\n",
    "        s = s.translate(c_dict)\n",
    "    return s\n",
    "\n",
    "def make_dict_cleaning(s, w_dict, skip_check=False):\n",
    "    # Replaces a word using dict if it is mutable\n",
    "    if skip_check or check_replace(s):\n",
    "        s = w_dict.get(s, s)\n",
    "    return s"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "## Get basic helper data\n",
    "\n",
    "bert_uncased_vocabulary = load_helper_file('helper_bert_uncased_vocabulary')\n",
    "bert_cased_vocabulary   = load_helper_file('helper_bert_cased_vocabulary')\n",
    "bert_char_list          = list(set([c for line in bert_uncased_vocabulary+bert_cased_vocabulary for c in line]))\n",
    "\n",
    "url_extensions          = load_helper_file('helper_url_extensions')\n",
    "html_tags               = load_helper_file('helper_html_tags')\n",
    "good_chars_dieter       = load_helper_file('helper_good_chars_dieter')\n",
    "bad_chars_dieter        = load_helper_file('helper_bad_chars_dieter')\n",
    "helper_contractions     = load_helper_file('helper_contractions')\n",
    "global_vocabulary       = load_helper_file('helper_global_vocabulary')\n",
    "global_vocabulary_chars = load_helper_file('helper_global_vocabulary_chars')\n",
    "normalized_chars        = load_helper_file('helper_normalized_chars')\n",
    "white_list_chars        = load_helper_file('helper_white_list_chars')\n",
    "white_list_punct        = \" '*-.,?!/:;_()[]{}<>=\" + '\"'\n",
    "pictograms_to_emoji     = load_helper_file('helper_pictograms_to_emoji')\n",
    "helper_custom_synonyms     = load_helper_file('helper_custom_synonyms')\n",
    "helper_currency_synonyms     = load_helper_file('helper_currency_synonyms')\n",
    "helper_custom_general_synonyms     = load_helper_file('helper_custom_general_synonyms')\n",
    "emoji_dict = set(e for lang in emoji.UNICODE_EMOJI.values() for e in lang)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "## Load Data\n",
    "good_cols       = ['_id', 'text']\n",
    "data = pd.read_parquet('../../data/bitcoin_twitter_test_raw/part_0.parquet')\n",
    "data = data.iloc[:20000][good_cols]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#################### Initial State:\n",
      "Unknown words: 75185 | Known words: 6991\n"
     ]
    }
   ],
   "source": [
    "## Start preprocessing\n",
    "texts = data['text']\n",
    "local_vocab = bert_uncased_vocabulary\n",
    "global_lower=True\n",
    "texts = texts.astype(str)\n",
    "if verbose: print('#' *20 ,'Initial State:'); check_vocab(texts, local_vocab)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Lowering everything:\n",
      "Unknown words: 65753 | Known words: 8352\n"
     ]
    }
   ],
   "source": [
    "def lower(texts):\n",
    "    texts = texts.apply(lambda x: x.lower())\n",
    "    if verbose: print('#'*10 ,'Step - Lowering everything:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "if global_lower:\n",
    "    texts = texts.pipe(lower)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize chars and dots:\n",
      "Unknown words: 65087 | Known words: 8387\n"
     ]
    }
   ],
   "source": [
    "# Normalize chars and dots - SEE HELPER FOR DETAILS\n",
    "def normalize_chars(texts):\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,normalized_chars) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: re.sub('\\(dot\\)', '.', x))\n",
    "    texts = texts.apply(lambda x: deaccent(x))\n",
    "    if verbose: print('#'*10 ,'Step - Normalize chars and dots:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Control Chars:\n",
      "Unknown words: 65087 | Known words: 8387\n"
     ]
    }
   ],
   "source": [
    "def remove_control_chars(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars_dict = {c:'' for c in global_chars_list if unicodedata.category(c)[0]=='C'}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#'*10 ,'Step - Control Chars:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_control_chars)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove hrefs:\n",
      "Unknown words: 65087 | Known words: 8387\n"
     ]
    }
   ],
   "source": [
    "def remove_hrefs(texts):\n",
    "    texts = texts.apply(lambda x: re.sub(re.findall(r'\\<a(.*?)\\>', x)[0], '', x) if (len(re.findall(r'\\<a (.*?)\\>', x))>0) and ('href' in re.findall(r'\\<a (.*?)\\>', x)[0]) else x)\n",
    "    if verbose: print('#'*10 ,'Step - Remove hrefs:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_hrefs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols:\n",
      "Unknown words: 64638 | Known words: 8422\n",
      "※自ठ収어接半𝓲壇操餐②析買延립処経様欲테대玲景베ะ倍式策怖邪眼電攻辛話請欧添票为悟皆𝒐逃♡調般𝐅末고늘내ヤ忘題説บ観尊何게겨技抗剰序突ｳ乱報禍暇触后起条｀録稼貼晩午𝐢瞬別印優痛𝗥掩ぇ‸効⇨只۵昇族리ண噂提ゃ嘘重封当在彻積۴𝐫상工𝟕系込興割∀录獲관携持𝐧病𝟰矿気含𝟬％본〆返読己射开深＜富♧範다ॉ‍既仕飲ヌ撤視数討ำ概縦税績정స放証记著引記竜폰昔座隆◡息減种¯獄找存黒他湧燃售械米悪忙艰٣集🇼𝗵복言総点결۱۲ｌ선𝒊適변克運検増ｱ孩告注⇩特臭𝓸略改違率న易赶뮤니煽ங強؟□申音拡焉貯보위慢甚寝終贫迎】科论𝗨만풀것泣个圧ช俺일採국換達𝓪商导ⓢ震块仲曜市番辿了静며整𝘆🇶格판❯𝐥対虎考ข١退𝗞即ｼ𝓷継攫私⬣勉］락七ｷ押었못있소低🇯ค標職案譯解決🇳降所埋낸毎就ﾃ詰列서회𝑶饮善慌ﾄ友為비質呟構ฒ她ৱ念ณ程遊亿ﾞ♤済像료으知万欺纯피昌需𝘂라ｅ要𝔂𝐊머네察頼스获穫確应値ﾛ魔𝟐东聡わ専限据騰裕倒실偉還＝제٠𝐆基厳ใ婴創𝗬眠議企價ｒ➤控🇹இ審무然額ด弾備吸０购숏非🇻可雰ﾊ백赚追번钟伴打𝒏은推凌ネৰ勧懸𝑷왕➥抑气ర시巨涨률教台共ญ営求◔錢🇦ｉ伸度無岸寒나호닐됴答辻₦焼脳早髭固慣択夭ซ僕催𝒄装급动組監勘조麻𝐑망𝗘𝗦🇬認𝐮管危알故坡𝗰期𝓭妄𝒉施더ｃ貸壁⃣形ฯ今코త를儲ﾀ𝑩𝐞ڡ利░𝟏커母節配余背激筆再響劇滞２跃오物域增至链予価ঘ停億兄兆周訴禁草너變褄論擬세昨肩색词課殺握吊원留客资ｸ등包𝓰해願忌𝓵약消𝓮切照औ데磨기以唯⣿🇱🇧￼찬厚位🇺🇨긴修ﾋ鷲流ﾌ端勢満ƀ室嫌堀筋売ﾗ𝐕連由透扶業들ফ唐＼詐想肯側인ภ央⁦इ율律𝐲借抽𝗛銀夜件影댓岁받投很直作ｯ徐路離旅芸材𝐄線斉約주儿용互𝟎수려굴密問練衆价講綺₺許ヶ払跳여先並𝘃入凄字院希独‥移并絡𝟱폐繋楽能締황ద동噴責ﾙａユ拉们𝐩設𝐚🇲움۷익轰장ｰ休純𝐒与寧拠警▓ｹ심制ソ完輪종似伺飛钱플医被素而帰死盖迷股暗𝐤유하킹𝟲習築５幅染招과樣着味銘乙戻員態𝐝詳迄化⟶談𝗲𝗶判애産거難ఇ妙ฟ现బ扱𝐁ｗﾎ吗広𝐌ฮ裏简로遣𝟵히亲界底丸ు通冊破両◝須係같🇭站１ﾟ圈态冬✦𝘄ﾘ值献歳𝗹熱躇紫瀧捉斯♪乗닝去候預𝐋販敗간牌𝗮۳힘⎌⁠登되𝐦向角𝗯交識จ每𝓱榴住残ﾏ緊𝗿奥訳霜도機＆失躊𝐔టไๆ撸额드달🇵恐変柄标評ｋ𝟓湾戒ォ貨準향実财拒拾購＋虛券彫瀚応板輩支得🇾声단𝐇緑홀ヨ蛇క命ぁ烟者识穩汇实份資類探那＃色夢ژ叶陰＞𝐭致託号۰◇脱币黄③波ถ導計抜威账株ฐ守聴历円最超◜多铁描యｺ祭𝗼￥短週롱薄近방𝐠騙置毒˘买습融법𝗢崩𝐬难승ธ団届＄𝗱走聞片型트罪𝓬使狙６布蘭普初运却望層茶收復７削₿抱鹿ผ軽任辺称난过弄散ణｲ除丁顆砲頃現ం堅進ป빼☞에更館마吃誰抵挖你𝑿암손𝗧𝐘🇸吴𝐎段▸極植［자首⁩取𝐡𝘀โ报萎悩喰将车送ょ喜터𝐀매急例۹載🇪酮充계雲順足暴好麗요協금저累理到운君性顔次⅒服查른𝓽感翻始约【歴𝐃ｍ損赤韓体酬宁𝘁帯誕鎖３＾𝐨𝐍９𝗡続給少🇽開场状均및鬼○各受毛否𝒕డ豪埃副益画泛伝織過敷及府𝓕𝐂其靴부열넷▲반常募그坛逆ⓜ品囲稿爆🇰浮𝗻亏ﾔ胸✧差‌塾ప測▼笑친货刺術지紹포従彼⠀旦；🇴🇿후謝𝓻𝐈𝑹参建惑桁₳豊큰費用ห量便貿创채𝒅思未△它🇫러貰況因疑結表露第象簡안배𝐜種幣없٪秩𝐓展ﾝ陆羨速冷傾을凉功回억가疲査义మ果呪时🇮頑肉𝟗錬𝗟負怀廻马窓絵户𝗠십炮화务納█付験単活𝒂勇意拍折ஆ뉴断越必来供指珠嬉티圳枚掘替源転時✓🇷림전労情雀絶全覇拳是局래悲関⋆微來動繰振물优ऑ排兜没々圏選แ匿規模𝓼待執𝟭細旧個試養刻►฿①反學港落�権는𝐟祝🇩녜아料算슬\n",
      "8251 --- \n",
      "33258 --- \n",
      "2336 --- \n",
      "21454 --- \n",
      "50612 --- \n",
      "25509 --- \n",
      "21322 --- \n",
      "120050 --- i\n",
      "22727 --- \n",
      "25805 --- \n"
     ]
    }
   ],
   "source": [
    "# Convert or remove Bad Symbols\n",
    "def convert_remove_bad_symbols(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if (c not in bert_char_list) and (c not in emoji_dict) and (c not in white_list_chars)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove Bad Symbols PART 2:\n",
      "Unknown words: 62759 | Known words: 8380\n",
      "·βσ加น。ت民₱へ目ふש仮前地示סフ」風野ω氏火ष宣山高तডつッ瀬←т光え√দワせ金ぬ『οウь小マ下長खד書ی石も学かםכ三あ真شφιिغী戦文京相場生าسצशпρレそ⇒』★י四女区見рфおツシذ上たオ이иอ春щγব立ョ信•的ทו「幸井чг）हطლीءתアپचκள…成ηงэмযんャ南ـ新白лケ良ोこνやоخュரノ比வ語分華व食नーみυろ！タপィমजןब本→มаカжقع（ш의אыהدеτ行国லস將ا花ध，زு€士スچض元ら朝ミج子─内平ך年力ζמ世พ月神у土তニب外むкگிث英юएى↑কวநதсコ사ひעगচडァ奈大ハ保有ةن代正男政டб●：цสε₹人イ馬र空ยר十千েט我天る・ট同車張会χ☆ルз之θ木チ↓？хはςசナ五ל原定م事な久二り発حヘλラ北ロ手ξにエα،सउே出ףגセو史明ホ陽う社谷।ム安めと心部ف義பريظнमв„都ゆร／れすล口也זपд和ちを止キ合中ल名家ก广後トহンリμבっ～াตாほ不星くヒाקけのदट島て美লい貴道法しモ日ェメअさமোகя州πサテδ〜一古كصねय한面لआ香主เ犬ま方חよি勝ه、き水門間ভ王ন海介کक≈クפ公נ\n",
      "183 --- \n",
      "946 --- \n",
      "963 --- \n",
      "21152 --- \n",
      "3609 --- \n",
      "12290 --- \n",
      "1578 --- \n",
      "27665 --- \n",
      "8369 --- \n",
      "12408 --- \n"
     ]
    }
   ],
   "source": [
    "# Remove Bad Symbols PART 2\n",
    "def convert_remove_bad_symbols2(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = '·' + ''.join([c for c in global_chars_list if (c not in white_list_chars) and (c not in emoji_dict) and (c not in white_list_punct) and (ord(c)>256)])\n",
    "    chars_dict = {}\n",
    "    for char in chars:\n",
    "        try:\n",
    "            new_char = unicodedata.name(char).split()[-1:][0].lower()\n",
    "            if len(new_char)==1:\n",
    "                chars_dict[ord(char)] = new_char\n",
    "            else:\n",
    "                chars_dict[ord(char)] = ''\n",
    "        except:\n",
    "            chars_dict[ord(char)] = ''\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove Bad Symbols PART 2:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_remove_bad_symbols2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - HTML tags:\n",
      "Unknown words: 62759 | Known words: 8380\n"
     ]
    }
   ],
   "source": [
    "def remove_html_tags(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if ('<' in word) and ('>' in word):\n",
    "            for tag in html_tags:\n",
    "                if ('<'+tag+'>' in word) or ('</'+tag+'>' in word):\n",
    "                    temp_dict[word] = BeautifulSoup(word, 'html5lib').text\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - HTML tags:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_html_tags)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 1:\n",
      "Unknown words: 51806 | Known words: 8380\n",
      "https://t.co/gelqggviyj --- @URL[t.co]\n",
      "https://t.co/cwogs38kcz --- @URL[t.co]\n",
      "https://t.co/mcvchqq4bu --- @URL[t.co]\n",
      "https://t.co/mgck6u5sfi --- @URL[t.co]\n",
      "https://t.co/zmcxs8ygxz --- @URL[t.co]\n",
      "https://t.co/r1cjnu7xqe --- @URL[t.co]\n",
      "https://t.co/rmekxunkpn --- @URL[t.co]\n",
      "https://t.co/zmcr4o0mps --- @URL[t.co]\n",
      "https://t.co/bfhipl2exl --- @URL[t.co]\n",
      "https://t.co/qjwvxvjscq --- @URL[t.co]\n",
      "########## Step - Convert urls part 1.5:\n",
      "Unknown words: 51805 | Known words: 8380\n"
     ]
    }
   ],
   "source": [
    "# Remove links (There is valuable information in links (probably you will find a way to use it))\n",
    "def remove_links(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    url_rule = r'(?P<url>https?://[^\\s]+)'\n",
    "    temp_dict = {k:domain_search(k) for k in temp_vocab if k!= re.compile(url_rule).sub('url', k)}\n",
    "\n",
    "    for word in temp_dict:\n",
    "        new_value = temp_dict[word]\n",
    "        if word.find('http')>2:\n",
    "            temp_dict[word] =  word[:word.find('http')] + ' ' + place_hold(new_value, URL_TAG)\n",
    "        else:\n",
    "            temp_dict[word] = place_hold(new_value, URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "\n",
    "    # Remove twitter urls\n",
    "    temp_dict = {\n",
    "        f'{URL_TAG}[t.co]': ''\n",
    "    }\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 1.5:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_links)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove escaped html:\n",
      "Unknown words: 51757 | Known words: 8380\n",
      "==&gt;&gt; --- ==\n",
      "s&amp;p --- s and p\n",
      "stakepool🥩&amp;🍳 --- stakepool🥩 and 🍳\n",
      "&gt;&gt;&gt;&gt;&gt; --- \n",
      "love&gt;money. --- lovemoney.\n",
      "&lt;$0.20. --- $0.20.\n",
      "p&amp;d --- p and d\n",
      "&gt;remember --- remember\n",
      "&gt;&gt;7 --- 7\n",
      "&lt;$f/s --- $f/s\n"
     ]
    }
   ],
   "source": [
    "# Remove escaped html\n",
    "def remove_escaped_html(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    symbols = {\n",
    "        '&quot;': '',\n",
    "        '&amp;': ' and ',\n",
    "        '&lt;': '',\n",
    "        '&gt;': '',\n",
    "    }\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if any([rep in word for rep in symbols.keys()]):\n",
    "            new_word = word\n",
    "            for rep, to in symbols.items():\n",
    "                new_word = new_word.replace(rep, to)\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove escaped html:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_escaped_html)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert urls part 2:\n",
      "Unknown words: 51754 | Known words: 8380\n",
      ".io/boxes/all?r=5f4c54b0bd312243977db0f7 --- @URL[url]\n",
      "httpss://betfury.io/boxes/all?r=601593e4b08af17cbc468064 --- @URL[betfury.io]\n",
      "35%/betfury.io/boxes/all?r=600b1be208cc0b1e47440365 --- @URL[url]\n",
      "www.studio192.nle --- @URL[studio192.nle]\n",
      "//t.co/nf0x22os7q --- @URL[url]\n"
     ]
    }
   ],
   "source": [
    "# Convert urls part 2\n",
    "def convert_urls2(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        url_check = False\n",
    "        if 'file:' in word:\n",
    "            url_check = True\n",
    "        elif ('http' in word) or ('ww.' in word) or ('.htm' in word) or ('ftp' in word) or ('.php' in word) or ('.aspx' in word):\n",
    "            if 'Aww' not in word:\n",
    "                for d_zone in url_extensions:\n",
    "                    if '.' + d_zone in word:\n",
    "                        url_check = True\n",
    "                        break\n",
    "        elif ('/' in word) and ('.' in word):\n",
    "            for d_zone in url_extensions:\n",
    "                if '.' + d_zone + '/' in word:\n",
    "                    url_check = True\n",
    "                    break\n",
    "\n",
    "        if url_check:\n",
    "            temp_dict[word] =  place_hold(domain_search(word), URL_TAG)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert urls part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_urls2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms:\n",
      "Unknown words: 51752 | Known words: 8380\n",
      ":-) --- 😁\n",
      ":)) --- 😁\n",
      ":))) --- 😁)\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>2:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if (pict in word) and (len(pict)>2):\n",
    "                    char_pict = pict[-1].isalpha() and pict[0].isalpha()\n",
    "                    if char_pict:\n",
    "                        pass\n",
    "                    else:\n",
    "                        temp_dict[word] = word.replace(pict, pictograms_to_emoji[pict])\n",
    "                elif pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate emoji:\n",
      "Unknown words: 50158 | Known words: 8404\n",
      "🩺🧐🔻📨🕴🍺🔃🌼⚠🍛⬆🐱💙📃🎢💞😇🏋🎾⁉⚙📝🌽❗🏤😝🌪☺☁❣🪄🍃📍👆◻😊🐾🏡😷⛳🐶🌠🔴🔟🪓↕☹🌂🎊💌🤎🤌🌵🐑👐💥🏠🛩💵🎬🔋🙌🖨🔍🚬⬛🕒👊🍊👹😵🖖🙋💝🥇🥵◼🐕▪🧨👋🔉💖😙🎇✔🕘🤲💻✳🏎🛥🛢🥷🖥🌛🏧🖼💘🚤⏲🚩👢🦥😛🔒🪀🚶🥴🤮😻🏀🧙🍰🛒☕◾🥈☑🤠📹💅⬅🔎🍯🌸🚀😳☀🚦📥⚫🤭🟠❌📱🟩📩🤙🔶🥚🤪📉💬🎶🏘🪜📤😹⛽🔹🌉💱📚🎥🏃🤢🛍💀🆙🌧😸⏰😎⌚😲🤹☮🌎🧑🌺💁🍇🍳✌👩😁👉🧳🧱🍕🖇😧😘🍾🖍👁🤨😤🔊😒🟧💯🙆🤑✈🤯⛑🐌®🤗🙀🌱🏾🦉🙈🤚🌒🧭🔮🍆🌙👺⏩🥳🛶🅰🙂🥂🐍⚖🍑💹🤟🏭🍻🎆💃⤵💰📡✖😄🌹📅🏆🔐🕷🏼🐐🧠🤡🟢👨🏈🧘👈🎙🌕📀☔🗓💤🌘🐋😶🔛🏁🎀➡🆘⏱✋🏿🕰📷🐢🗣💓💊😢🏽🐯🤬☃🍎💡💟❄🚨📢🔜🐀🆕☝🧊🎧🎉⛔🌐😐🐂🍄🏊⛏📗🥉▫⛄🐭👌🎯🟡🥙⬇🍬😯🌋〽💭📯🔄🦳⛵💶💩👧💳😭‼🛳🩳🐻👟🚂©🤺💕👦🌚🍌🚪📽🖌👓🙊🐃📋🧡📣🤾💦🕯💲🗼🛫🐺🙉🦄🥩🏞🤸🔦♾❕😖🤞🔫🎂♻🔘🐛🌗😰🏴🌝👽📸💧🏻🪙🛑🐏📲📬😉🎸🐓🎱🤖📦⏬🤣🥺🌴🐦😨♥🌜ℹ⏳🐝🦃💋🔵🟨🌖👤😋☠😫🤛📰㊗💛📌✍🐸🎈💴🏦🚛😱✊🍁🦆🤷😈⛷⌛🥃✅🖕⤴👛🍀🤘🆒🌟🧬🃏🍿📏🙄🏄🔗🌏🥰🗽🌮🥶☄♦👏👅🐙🎐😪🗞☣▶🔷💗🔰🌇😬🤫🍷🏇🏜™🐰♀🙇💫⚛🚘🔽🥁🌄📈😕🌑🏒🤴🏓🐊🕐🍔🦠🐬😮🌅❤🪨🤏🦾🏅🐇✨🌈💪👍🎁🔸🦰🚌〰🪐🛸📪🌡🆓🎻🌞👇🙏🐳😴🐎🏰📆💣🤦🕹👂🔝🤓🌊🤤💐💜⛴♂😀🥞🐧🏛🥱💠😑🤝♣🤔😓🚗🐟😥📊🩸💿💨😣👑💉👔💔🎼🍦🧲⚡🖐🤧😃❓🧯😌🏪🔥🌔🥲🏖🦍♠🧿🕸🎦🏮🦅🔼☘😅😔🔖📄👎🏌🛠🤩💚🙃⭐🔺🔔🐮🅱🧻🛰😍💎❇😡💸🕚😏🎨⛓😜🌓👀🎤🤳🔞😽🛡📺🚄🖤🌌🌍😂🦜😚🗯⚽😆🕵🛎🎣⭕\n"
     ]
    }
   ],
   "source": [
    "def isolate_emoji(texts):\n",
    "    global_chars_list = list(set([c for line in texts for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if c in emoji_dict])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate emoji:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print(chars)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_emoji)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Duplicated Chars:\n",
      "Unknown words: 48775 | Known words: 8452\n"
     ]
    }
   ],
   "source": [
    "# Duplicated dots, question marks and exclamations\n",
    "def deduplicate_dots(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if (Counter(word)['.']>1) or (Counter(word)['!']>1) or (Counter(word)['?']>1) or (Counter(word)[',']>1):\n",
    "            if (Counter(word)['.']>1):\n",
    "                new_word = re.sub('\\.\\.+', ' . . . ', new_word)\n",
    "            if (Counter(word)['!']>1):\n",
    "                new_word = re.sub('\\!\\!+', ' ! ! ! ', new_word)\n",
    "            if (Counter(word)['?']>1):\n",
    "                new_word = re.sub('\\?\\?+', ' ? ? ? ', new_word)\n",
    "            if (Counter(word)[',']>1):\n",
    "                new_word = re.sub('\\,\\,+', ' , , , ', new_word)\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Duplicated Chars:'); check_vocab(texts, local_vocab);\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(deduplicate_dots)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove underscore:\n",
      "Unknown words: 48762 | Known words: 8452\n",
      "#_ --- #\n",
      "__________________ --- \n",
      "___! --- !\n",
      "@l_____l____l___ --- @lll\n",
      "____ --- \n",
      "_______ --- \n",
      "_____ --- \n",
      "______.\" --- .\"\n",
      "webd____________________ --- webd\n",
      "#_l --- #l\n"
     ]
    }
   ],
   "source": [
    "# Remove underscore for spam words\n",
    "def remove_underscore_spam(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and ('_' in word):\n",
    "            temp_dict[word] = re.sub('_', '', word)\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_underscore_spam)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Spam chars repetition:\n",
      "Unknown words: 48752 | Known words: 8452\n",
      "$$$$$ ---  $ \n",
      "*** ---  * \n",
      "**** ---  * \n",
      "$$$ ---  $ \n",
      "************** ---  * \n",
      "^^^^ ---  ^ \n",
      "$$$$ ---  $ \n",
      "*************** ---  * \n",
      "^^^^^ ---  ^ \n",
      "^^^ ---  ^ \n"
     ]
    }
   ],
   "source": [
    "# Isolate spam chars repetition\n",
    "def isolate_spam_characters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(re.compile('[a-zA-Z0-9\\-\\.\\,\\/\\']').sub('', word))/len(word) > 0.6) and (len(Counter(word))==1) and (len(word)>2):\n",
    "            temp_dict[word] = ' '.join([' ' + next(iter(Counter(word).keys())) + ' ' for i in range(1)])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Spam chars repetition:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_spam_characters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Normalize pictograms part 2:\n",
      "Unknown words: 48746 | Known words: 8452\n",
      ":) --- 😁\n",
      "=) --- 😁\n",
      ":( --- 😡\n",
      ";) --- 😜\n",
      "%) --- 😵\n",
      ":/ --- 🤔\n"
     ]
    }
   ],
   "source": [
    "# Normalize pictograms part 2\n",
    "# Local (only unknown words)\n",
    "def normalize_pictograms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9]').sub('', word))>1:\n",
    "            for pict in pictograms_to_emoji:\n",
    "                if pict==word:\n",
    "                    temp_dict[word] = pictograms_to_emoji[pict]\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Normalize pictograms part 2:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(normalize_pictograms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Brackets and quotes:\n",
      "Unknown words: 46987 | Known words: 8523\n",
      "40 ---  ( \n",
      "41 ---  ) \n",
      "91 ---  [ \n",
      "93 ---  ] \n",
      "123 ---  { \n",
      "125 ---  } \n",
      "60 ---  < \n",
      "62 ---  > \n",
      "34 ---  \" \n"
     ]
    }
   ],
   "source": [
    "# Isolate brakets and quotes\n",
    "def isolate_brackets(texts):\n",
    "    chars = '()[]{}<>\"'\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_cleaning(i,chars_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Brackets and quotes:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(chars_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_brackets)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Extract date and time:\n",
      "Unknown words: 46989 | Known words: 8523\n",
      "11:12:43 ---  @TIME[11:12:43] \n",
      "06:26:31 ---  @TIME[06:26:31] \n",
      "17:05:00 ---  @TIME[17:05:00] \n",
      "02/01/2021, ---  @DATE[02/01/2021] \n",
      "23:12:20 ---  @TIME[23:12:20] \n",
      "02/02/2021 ---  @DATE[02/02/2021] \n",
      "13:17:53 ---  @TIME[13:17:53] \n",
      "13:12:47 ---  @TIME[13:12:47] \n",
      "14:19:13 ---  @TIME[14:19:13] \n",
      "06:02:43 ---  @TIME[06:02:43] \n"
     ]
    }
   ],
   "source": [
    "# Extract date and time\n",
    "def extract_date_and_time(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_fix = re.compile('^[$£%€][-+][0-9]')\n",
    "    time_regex = re.compile('([0-9]{1,2}:[0-9]{1,2}:[0-9]{1,4})')\n",
    "    date_regex = re.compile('([0-9]{1,4}\\/[0-9]{1,2}\\/[0-9]{1,4})')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "\n",
    "        ## -------- Time\n",
    "        time_result = time_regex.search(prefilter)\n",
    "        if time_result:\n",
    "            prefix = prefilter[:time_result.start()]\n",
    "            suffix = prefilter[time_result.end():]\n",
    "            mpart = prefilter[time_result.start():time_result.end()]\n",
    "            temp_dict[word] = ' '.join([\n",
    "                prefix,\n",
    "                place_hold(str(mpart), TIME_TAG),\n",
    "                suffix\n",
    "            ])\n",
    "            continue\n",
    "\n",
    "        ## -------- Date\n",
    "        date_result = date_regex.search(prefilter.replace('-', '/'))\n",
    "        if date_result and len(word.split('/')) == 3:\n",
    "            prefix = prefilter[:date_result.start()]\n",
    "            suffix = prefilter[date_result.end():]\n",
    "            mpart = prefilter[date_result.start():date_result.end()]\n",
    "            temp_dict[word] = ' '.join([\n",
    "                prefix,\n",
    "                place_hold(str(mpart), DATE_TAG),\n",
    "                suffix\n",
    "            ])\n",
    "            continue\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Extract date and time:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_date_and_time)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 46985 | Known words: 8523\n",
      "b4 --- before\n",
      "mkt --- market\n",
      "u.s. --- united states\n",
      "chg --- change\n"
     ]
    }
   ],
   "source": [
    "def custom_global_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_general_synonyms:\n",
    "            temp_dict[word] = helper_custom_general_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom global word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_global_synonyms)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break short words:\n",
      "Unknown words: 46617 | Known words: 8546\n",
      "16th/s --- 16th / s\n",
      "6/10 --- 6 / 10\n",
      "p/e --- p / e\n",
      "44/100 --- 44 / 100\n",
      "#arpa/#btc --- #arpa / #btc\n",
      "2021/clubhouse --- 2021 / clubhouse\n",
      "$theta/ --- $theta / \n",
      "76/100 --- 76 / 100\n",
      "/r/wallstreetbets, ---  / r / wallstreetbets,\n",
      "4/ --- 4 / \n"
     ]
    }
   ],
   "source": [
    "# Break short words\n",
    "def break_short_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)<=20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break short words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(break_short_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Break long words:\n",
      "Unknown words: 46595 | Known words: 8551\n",
      "ripconnnneeeeecccctt. --- ripconnnneeeeecccctt . \n",
      "trx,doge,xrp,kava,xlm,bel,cvc --- trx , doge , xrp , kava , xlm , bel , cvc\n",
      "poker/blackjack/roulette --- poker / blackjack / roulette\n",
      "sell-bitcoin-btc-for-usd-in-united-states --- sell bitcoin btc for usd in united states\n",
      "everybody's-getting-#bitcoin-for-birthdays --- everybody's getting #bitcoin for birthdays\n",
      "noche/madrugada/manana --- noche / madrugada / manana\n",
      "buy-bitcoin-btc-for-aud-in-kenya --- buy bitcoin btc for aud in kenya\n",
      "34650-34850-35050-35250-35450 --- 34650 34850 35050 35250 35450\n",
      "better,faster,chesper --- better , faster , chesper\n",
      "range:01/01/2016-02/01/2021 --- range : 01/01/2016-02/01/2021\n",
      "########## Step - Break long words:\n",
      "Unknown words: 46594 | Known words: 8551\n",
      "reddit/robinhood/gamestop --- reddit / robinhood / gamestop\n",
      "level,thankyouverymuch --- level , thankyouverymuch\n",
      "01/01/2016-02/01/2021 --- 01 / 01 / 2016-02 / 01 / 2021\n",
      "########## Step - Break long words:\n",
      "Unknown words: 46594 | Known words: 8551\n"
     ]
    }
   ],
   "source": [
    "# Break long words\n",
    "def break_long_words(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_vocab = [k for k in temp_vocab if len(k)>20]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if '_' in word and not (len(word) > 2 and word[0] in ['#', '$', '@'] and word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            temp_dict[word] = re.sub('_', ' ', word)\n",
    "        elif '/' in word and not word.startswith('u/') and not word.startswith('r/'):\n",
    "            temp_dict[word] = re.sub('/', ' / ', word)\n",
    "        elif len(' '.join(word.split('-')).split())>2:\n",
    "            temp_dict[word] = re.sub('-', ' ', word)\n",
    "        for s in ',.:;':\n",
    "            if s in word and not re.compile('[+#@$/,.:;-]').sub('', word).isnumeric():\n",
    "                temp_dict[word] = word.replace(s, f' {s} ')\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Break long words:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "for i in range(3):\n",
    "    texts = texts.pipe(break_long_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 46502 | Known words: 8553\n",
      ",#xrp --- , #xrp\n",
      ".#online --- . #online\n",
      "12#in --- 12 #in\n",
      ".@elonmusk --- . @elonmusk\n",
      "goal.#nextprotocol --- goal. #nextprotocol\n",
      "us$35 --- us $35\n",
      "iui#btc --- iui #btc\n",
      "1.61-$11 --- 1.61- $11\n",
      "~$35k --- ~ $35k\n",
      "ai#btc --- ai #btc\n"
     ]
    }
   ],
   "source": [
    "# TODO: add number parsing before\n",
    "# Diambiguate entities\n",
    "# Split words on @,# and $ to clear up ambiguities between entitites\n",
    "def disambiguate_entitites(texts):\n",
    "    symbols = '@#$'\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('@' in k or '#' in k or '$' in k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        for symbol in symbols:\n",
    "            if symbol not in word: continue\n",
    "            left, *right = word.split(symbol)\n",
    "            rightz = symbol.join(right)\n",
    "            if len(left) > 0 and len(right[0]) > 0 and right[0].isalnum():\n",
    "                temp_dict[word] = f'{left} {symbol}{rightz}'\n",
    "            break\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Disambiguate entities:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(disambiguate_entitites)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 46463 | Known words: 8553\n",
      "$hodl --- #hodl\n",
      "bitstamp --- @bitstamp\n",
      "@blockchain --- #blockchain\n",
      "coinbase --- @coinbase\n",
      "#bittrex --- @bittrex\n",
      "paypal --- @paypal\n",
      "#coinbase --- @coinbase\n",
      "$binance --- @binance\n",
      "#bitmex --- @bitmex\n",
      "#altcoin --- #altcoins\n"
     ]
    }
   ],
   "source": [
    "def custom_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_custom_synonyms:\n",
    "            temp_dict[word] = helper_custom_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom word synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 46173 | Known words: 8553\n",
      "comp --- $compound\n",
      "$cro --- $crypto_com_coin\n",
      "$cas --- $cashaa\n",
      "$nmc --- $namecoin\n",
      "yfi --- $yearn_finance\n",
      "$nav --- $nav_coin\n",
      "$audio --- $audius\n",
      "#qtum --- $qtum\n",
      "$orn --- $orion_protocol\n",
      "$cover --- $cover_protocol_new\n"
     ]
    }
   ],
   "source": [
    "def custom_currency_synonyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_currency_synonyms:\n",
    "            temp_dict[word] = helper_currency_synonyms[word]\n",
    "\n",
    "    for k,v in list(temp_dict.items()):\n",
    "        if k == v:\n",
    "            temp_dict.pop(k)\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Custom currency synonyms:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(custom_currency_synonyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 45459 | Known words: 8553\n",
      "@cardosogerman --- @USR[cardosogerman]\n",
      "#binanceexchange --- @HTAG[binanceexchange]\n",
      "@followarmysbts --- @USR[followarmysbts]\n",
      "#speculators --- @HTAG[speculators]\n",
      "#decentralization --- @HTAG[decentralization]\n",
      "$uvxy --- @HTAG[uvxy]\n",
      "#bigpumpsignal --- @HTAG[bigpumpsignal]\n",
      "@bloomberg --- @USR[bloomberg]\n",
      "#casino --- @HTAG[casino]\n",
      "#financialeducation --- @HTAG[financialeducation]\n"
     ]
    }
   ],
   "source": [
    "# Remove/Convert usernames and hashtags\n",
    "def extract_entities(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (len(word) > 2) and (word[1:len(word)-1].replace('\\'s', '').replace('_', '').isalnum()):\n",
    "            new_word = word.replace('\\'s', '')\n",
    "            if not re.compile('[#@$/,.:;]').sub('', new_word).isnumeric():\n",
    "                new_word = re.compile('[,.:;]').sub('', new_word)\n",
    "                if word.startswith('@'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], USER_TAG)\n",
    "                elif word.startswith('#'):\n",
    "                    temp_dict[word] = place_hold(new_word[1:], HASH_TAG)\n",
    "                elif word.startswith('u/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], USER_TAG)\n",
    "                elif word.startswith('r/'):\n",
    "                    temp_dict[word] = place_hold(new_word[2:], HASH_TAG)\n",
    "                elif word.startswith('$') and new_word[1:].replace('_', '').isalpha():\n",
    "                    tag = CURRENCY_TAG if word[1:] in helper_currency_synonyms else HASH_TAG\n",
    "                    temp_dict[word] = place_hold(new_word[1:], tag)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - UserName and Hashtag:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(extract_entities)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 45432 | Known words: 8553\n",
      "@HTAG[link] --- @CURR[link]\n",
      "@HTAG[xrp] --- @CURR[xrp]\n",
      "@HTAG[zilliqa] --- @CURR[zilliqa]\n",
      "@HTAG[qtum] --- @CURR[qtum]\n",
      "@HTAG[dash] --- @CURR[dash]\n",
      "@HTAG[litecoin] --- @CURR[litecoin]\n",
      "@HTAG[tezos] --- @CURR[tezos]\n",
      "@HTAG[bitcoin] --- @CURR[bitcoin]\n",
      "@USR[bitcoin] --- @CURR[bitcoin]\n",
      "@HTAG[cardano] --- @CURR[cardano]\n"
     ]
    }
   ],
   "source": [
    "# Hashtag and currency union\n",
    "def hashtag_currency_union(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = set([k for k in temp_vocab if not check_replace(k)])\n",
    "    temp_dict = {}\n",
    "    for w in temp_vocab:\n",
    "        if w.startswith(CURRENCY_TAG):\n",
    "            if w.replace(CURRENCY_TAG, HASH_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, HASH_TAG)] = w\n",
    "            if w.replace(CURRENCY_TAG, USER_TAG) in temp_vocab:\n",
    "                temp_dict[w.replace(CURRENCY_TAG, USER_TAG)] = w\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict, skip_check=True) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Hashtag and currency union:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove ending underscore:\n",
      "Unknown words: 45431 | Known words: 8553\n",
      "zennf_ --- zennf\n",
      "_h_o_u_r_s_ --- _h_o_u_r_s\n",
      "_i_n_ --- _i_n\n",
      "._ --- .\n",
      "e_n_d_s_ --- e_n_d_s\n"
     ]
    }
   ],
   "source": [
    "# Remove ending underscore (or add quotation marks???)\n",
    "def remove_ending_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[len(word)-1]=='_':\n",
    "            for i in range(len(word),0,-1):\n",
    "                if word[i-1]!='_':\n",
    "                    new_word = word[:i]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove ending underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_ending_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove starting underscore:\n",
      "Unknown words: 45431 | Known words: 8553\n",
      "_h_o_u_r_s --- h_o_u_r_s\n",
      "_i_n --- i_n\n"
     ]
    }
   ],
   "source": [
    "# Remove starting underscore\n",
    "def remove_starting_underscore(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('_' in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        if word[0]=='_':\n",
    "            for i in range(len(word)):\n",
    "                if word[i]!='_':\n",
    "                    new_word = word[i:]\n",
    "                    temp_dict[word] = new_word\n",
    "                    break\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove starting underscore:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_starting_underscore)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - End word punctuations:\n",
      "Unknown words: 37616 | Known words: 9040\n",
      "crypto: --- crypto :\n",
      "mind-bender, --- mind-bender ,\n",
      "clue. --- clue .\n",
      "rappers, --- rappers ,\n",
      "'21, --- '21 ,\n",
      "'stake' --- 'stake '\n",
      "informations. --- informations .\n",
      "meme! --- meme !\n",
      "semana. --- semana .\n",
      "fund. --- fund .\n"
     ]
    }
   ],
   "source": [
    "# End word punctuations\n",
    "def end_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[len(k)-1].isalnum())]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word),0,-1):\n",
    "            if word[i-1].isnumeric() and re.compile('[$£%€]').match(word[i]):\n",
    "                break\n",
    "\n",
    "            if word[i-1].isalnum():\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - End word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(end_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Serialize numbers:\n",
      "Unknown words: 36070 | Known words: 9052\n",
      "$1488.36 --- @NUM[1488.36] usd\n",
      "4.63% --- @NUM[4.63] percent\n",
      "0xce1f27b591ca205066ac9257e3cab7b604a457b4 --- @NUM[0.0] xce1f27b591ca205066ac9257e3cab7b604a457b4\n",
      "$28k --- @NUM[28000.0] usd\n",
      "193.50$ --- @NUM[193.5] usd\n",
      "424,017 --- @NUM[424017.0]\n",
      "171.18$ --- @NUM[171.18] usd\n",
      "1.63% --- @NUM[1.63] percent\n",
      "2k --- @NUM[2000.0]\n",
      "44,903 --- @NUM[44903.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35971 | Known words: 9052\n",
      "2k --- @NUM[2000.0]\n",
      "0.00145 --- @NUM[145.0]\n",
      "17% --- @NUM[17.0] percent\n",
      "0.45 --- @NUM[0.45]\n",
      "2022 --- @NUM[2022.0]\n",
      "+35 --- @NUM[35.0]\n",
      "0.00135 --- @NUM[135.0]\n",
      "0.0014 --- @NUM[14.0]\n",
      "10pm --- @NUM[10.0] pm\n",
      "21,621 --- @NUM[21621.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35968 | Known words: 9052\n",
      "+5x --- @NUM[5.0] times\n",
      "=72k --- equals @NUM[72000.0]\n",
      "=1.500 --- equals @NUM[1500.0]\n",
      "=11k --- equals @NUM[11000.0]\n",
      "*365 --- @NUM[365.0]\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35968 | Known words: 9052\n"
     ]
    }
   ],
   "source": [
    "scale_mapping = {\n",
    "    'b': 1000000000,\n",
    "    'bn': 1000000000,\n",
    "    'bln': 1000000000,\n",
    "    'billion': 1000000000,\n",
    "    'm': 1000000,\n",
    "    'mn': 1000000,\n",
    "    'mln': 1000000,\n",
    "    'million': 1000000,\n",
    "    'k': 1000,\n",
    "    'thousand': 1000,\n",
    "    '-': -1,\n",
    "}\n",
    "\n",
    "translate = {\n",
    "    '$': 'usd', '£': 'gbp','%': 'percent', '€': 'eur'\n",
    "}\n",
    "\n",
    "translate_suffix = {\n",
    "    'x': 'times'\n",
    "}\n",
    "\n",
    "translate_prefix = {\n",
    "    '~': 'around',\n",
    "    '+-': 'around',\n",
    "    '±': 'around',\n",
    "    '@': 'at',\n",
    "    '=': 'equals',\n",
    "    '*#': 'ranked',\n",
    "    '#': 'ranked',\n",
    "}\n",
    "\n",
    "def serialize_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    re_inb = re.compile('[,\\'\"`]')\n",
    "    re_num = re.compile('^(~|\\+-|±|@|=|#|\\*#)?[-@+*^#:]?[$£%€]?(([.:]?[0-9])+)[$£%€]?')\n",
    "    re_fix = re.compile('^[$£%€][-+][0-9]')\n",
    "    time_regex = re.compile('([0-9]{1,2}:[0-9]{1,2}:[0-9]{1,4})')\n",
    "    date_regex = re.compile('([0-9]{1,4}\\/[0-9]{1,2}\\/[0-9]{1,4})')\n",
    "    for word in temp_vocab:\n",
    "        prefilter = re_inb.sub('', word).replace(',', '.')\n",
    "        if re_fix.search(prefilter):\n",
    "            prefilter = prefilter[1] + prefilter[0] + prefilter[2:]\n",
    "\n",
    "        ## ----- Various other numbers\n",
    "        result = re_num.search(prefilter)\n",
    "        if result and result.pos == 0:\n",
    "            # Process combined numbers / ranges in next iteration\n",
    "            if '-' in word and not word.startswith('-') and not word.startswith('+-'):\n",
    "                temp_dict[word] = ' '.join(word.split('-'))\n",
    "                continue\n",
    "\n",
    "            main_part = prefilter[:result.end()]\n",
    "            prefix = ''\n",
    "            for prefix_key, prefix_name in translate_prefix.items():\n",
    "                if main_part.startswith(prefix_key):\n",
    "                    prefix = prefix_name\n",
    "                    main_part = main_part.replace(prefix_key, '', 1)\n",
    "                    break\n",
    "\n",
    "            main = re.compile('^[~@+*^#:]').sub('',main_part)\n",
    "            currency = re.compile('[$£%€]').search(main)\n",
    "            currency = main[currency.start():currency.end()] if currency else None\n",
    "            main = re.compile('[$£%€]').sub('', main)\n",
    "            suffix = prefilter[result.end():]\n",
    "\n",
    "            multiplier = 1\n",
    "            if re.compile('\\.[0-9]{1,2}$').search(main): # decimal\n",
    "                multiplier *= 0.01 if main[-1].isnumeric() else 0.1\n",
    "            if '-' in main: # Neg numbers\n",
    "                multiplier *= -1\n",
    "                main = main.replace('-', '')\n",
    "            # Textual scale\n",
    "            if suffix in scale_mapping:\n",
    "                multiplier *= scale_mapping[suffix]\n",
    "                suffix = ''\n",
    "            if suffix in translate_suffix:\n",
    "                suffix = translate_suffix[suffix]\n",
    "\n",
    "            number = round(float(main.replace('.', '').replace(':', '')) * multiplier, 2)\n",
    "            # print(f'{number}  /  {currency}  /  {suffix}  /  {word}')\n",
    "            # noinspection PyTypeChecker\n",
    "            temp_dict[word] = ' '.join(filter(len,[\n",
    "                prefix,\n",
    "                place_hold(str(number), NUMBER_TAG),\n",
    "                translate[currency] if currency else '',\n",
    "                suffix\n",
    "            ]))\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Serialize numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "\n",
    "# Clean up numbers\n",
    "for i in range(4):\n",
    "    texts = texts.pipe(serialize_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 35967 | Known words: 9052\n",
      "chg --- change\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 35968 | Known words: 9052\n",
      ".@elonmusk --- . @elonmusk\n",
      "#innitialcoinoffering@myidentitycoin --- #innitialcoinoffering @myidentitycoin\n",
      "'#bitcoin --- ' #bitcoin\n",
      "|#litecoin --- | #litecoin\n",
      ".@moneyonchainok --- . @moneyonchainok\n",
      "us$450 --- us $450\n",
      ".@peterschiff --- . @peterschiff\n",
      "guap@s --- guap @s\n",
      ".@joebiden --- . @joebiden\n",
      ".@jack --- . @jack\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 35964 | Known words: 9052\n",
      "coinbase --- @coinbase\n",
      "paypal --- @paypal\n",
      "cointelegraph --- @cointelegraph\n",
      "binance --- @binance\n",
      "airdrop --- #airdrop\n",
      "altcoin --- #altcoins\n",
      "blockchain --- #blockchain\n",
      "bittrex --- @bittrex\n",
      "dogecoins --- $dogecoin\n",
      "#dogecoins --- $dogecoin\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 35942 | Known words: 9052\n",
      "yfi --- $yearn_finance\n",
      "#bitcoin --- $bitcoin\n",
      "jpy --- $jpy\n",
      "$gme --- $gamestop_tokenized_stock_ftx\n",
      "uniswap --- $uniswap\n",
      "$mrph --- $morpheus_network\n",
      "$grt --- $golden_ratio_token\n",
      "aave --- $aave\n",
      "$doge --- $dogecoin\n",
      "ltc --- $litecoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 35865 | Known words: 9052\n",
      "$usd --- @CURR[usd]\n",
      "$jpy --- @CURR[jpy]\n",
      "@joebiden --- @USR[joebiden]\n",
      "$vechain --- @CURR[vechain]\n",
      "$tezos --- @CURR[tezos]\n",
      "$hard_protocol --- @HTAG[hard_protocol]\n",
      "$usd_coin --- @CURR[usd_coin]\n",
      "$eur --- @CURR[eur]\n",
      "$digibyte --- @CURR[digibyte]\n",
      "$binance_coin --- @CURR[binance_coin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 35865 | Known words: 9052\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Start word punctuations:\n",
      "Unknown words: 35558 | Known words: 9069\n",
      "'but --- ' but\n",
      "'pragmatic --- ' pragmatic\n",
      "¥237500.02 --- ¥ 237500.02\n",
      "¿posee --- ¿ posee\n",
      "'units --- ' units\n",
      "*valentine --- * valentine\n",
      "¿cuales --- ¿ cuales\n",
      "-s --- - s\n",
      "'gone --- ' gone\n",
      "***we --- *** we\n"
     ]
    }
   ],
   "source": [
    "# Start word punctuations\n",
    "def start_word_punctuations(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (not k[0].isalnum() and k[0] not in ['@', '#', '$'])]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = word\n",
    "        for i in range(len(word)):\n",
    "            if word[i].isalnum() or word[i] in ['#', '@', '$']:\n",
    "                new_word = word[:i] + ' ' + word[i:]\n",
    "                break\n",
    "        temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Start word punctuations:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(start_word_punctuations)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 35558 | Known words: 9069\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 35558 | Known words: 9069\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35555 | Known words: 9069\n",
      "3653341.0 --- @NUM[365334.1]\n",
      "218011.21 --- @NUM[218011.21]\n",
      "224373.40 --- @NUM[224373.4]\n",
      "3,767,132 --- @NUM[3767132.0]\n",
      "3631924.0 --- @NUM[363192.4]\n",
      "217126.98 --- @NUM[217126.98]\n",
      "225519.82 --- @NUM[225519.82]\n",
      "3667909.0 --- @NUM[366790.9]\n",
      "$700,283.22 --- @NUM[700283.22] usd\n",
      "$733,005.64 --- @NUM[733005.64] usd\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 35555 | Known words: 9069\n",
      "cryptocurrency --- #cryptocurrency\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 35555 | Known words: 9069\n",
      "ethereum --- $ethereum\n",
      "bitcoin --- $bitcoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 35549 | Known words: 9069\n",
      "@_cryptocurator --- @USR[_cryptocurator]\n",
      "$ethereum --- @CURR[ethereum]\n",
      "@flow_blockchain --- @USR[flow_blockchain]\n",
      "@iohk_charles --- @USR[iohk_charles]\n",
      "@michael_saylor --- @USR[michael_saylor]\n",
      "#cryptocurrency --- @HTAG[cryptocurrency]\n",
      "$bitcoin --- @CURR[bitcoin]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 35549 | Known words: 9069\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Find and replace acronims:\n",
      "Unknown words: 35549 | Known words: 9069\n",
      "a.k.a --- word_placeholder[aka]\n",
      "b.a.l --- word_placeholder[bal]\n"
     ]
    }
   ],
   "source": [
    "# Find and replace acronims\n",
    "def find_replace_acronyms(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if (Counter(word)['.']>1) and (check_replace(word)):\n",
    "            if (domain_search(word)!='') and (('www' in word) or (Counter(word)['/']>3)):\n",
    "                temp_dict[word] = place_hold('url ' + domain_search(word))\n",
    "            else:\n",
    "                if (re.compile('[\\.\\,]').sub('', word) in local_vocab) and (len(re.compile('[0-9\\.\\,\\-\\/\\:]').sub('', word))>0):\n",
    "                    temp_dict[word] =  place_hold(re.compile('[\\.\\,]').sub('', word))\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Find and replace acronims:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(find_replace_acronyms)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Contractions:\n",
      "Unknown words: 35486 | Known words: 9069\n",
      "let's --- let us\n",
      "ya'll --- you will\n",
      "that'll --- that will\n",
      "would've --- would have\n",
      "i've --- i have\n",
      "i'll --- i will\n",
      "you're --- you are\n",
      "wouldn't --- would not\n",
      "you've --- you have\n",
      "weren't --- were not\n"
     ]
    }
   ],
   "source": [
    "# Apply spellchecker for contractions\n",
    "def apply_spellchecker_contractions(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (\"'\" in k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if word in helper_contractions:\n",
    "            temp_dict[word] = helper_contractions[word] # place_hold(helper_contractions[word])\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Contractions:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(apply_spellchecker_contractions)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Remove \"s:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "portfolio's --- portfolio\n",
      "rat's --- rat\n",
      "btc's --- btc\n",
      "@elonmusk's --- @elonmusk\n",
      "hl's --- hl\n",
      "bba's --- bba\n",
      "occam's --- occam\n",
      "argentina's --- argentina\n",
      "@zackvoell's --- @zackvoell\n",
      "schokobub's --- schokobub\n"
     ]
    }
   ],
   "source": [
    "# Remove 's (DO WE NEED TO REMOVE IT???)\n",
    "def remove_comma_s(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {k:k[:-2] for k in temp_vocab if (check_replace(k)) and (k.lower()[-2:]==\"'s\")}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Remove \"s:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_comma_s)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Convert backslash:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "#btc\\#usdt --- #btc / #usdt\n"
     ]
    }
   ],
   "source": [
    "def convert_backslash(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and ('\\\\' in k)]\n",
    "    temp_dict = {k:re.sub('\\\\\\\\+', ' / ', k) for k in temp_vocab}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Convert backslash:'); check_vocab(texts, local_vocab)\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_backslash)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 35278 | Known words: 9084\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 35275 | Known words: 9084\n",
      "paypal --- @paypal\n",
      "binance --- @binance\n",
      "kraken --- @kraken\n",
      "blockchain --- #blockchain\n",
      "cryptocurrency --- #cryptocurrency\n",
      "crypto --- #cryptocurrency\n",
      "hodl --- #hodl\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 35266 | Known words: 9084\n",
      "#bitcoin --- $bitcoin\n",
      "$tsla --- $tesla_tokenized_stock_bittrex\n",
      "$gme --- $gamestop_tokenized_stock_ftx\n",
      "eth --- $ethereum\n",
      "#usdt --- $tether\n",
      "#btc --- $bitcoin\n",
      "$akro --- $akropolis\n",
      "#quant --- $quant\n",
      "#ethereum --- $ethereum\n",
      "litecoin --- $litecoin\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 35212 | Known words: 9084\n",
      "@tesla --- @USR[tesla]\n",
      "@kraken --- @USR[kraken]\n",
      "@knutsvanholm --- @USR[knutsvanholm]\n",
      "@razor_network --- @USR[razor_network]\n",
      "@virgilgr --- @USR[virgilgr]\n",
      "@trustologyio --- @USR[trustologyio]\n",
      "$cardano --- @CURR[cardano]\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "$tron --- @CURR[tron]\n",
      "@elonmusk --- @USR[elonmusk]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 35212 | Known words: 9084\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Dup chars (with vocab check):\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "uu --- u\n",
      "ioo --- io\n",
      "jeet --- jet\n",
      "bbs --- bs\n",
      "upp --- up\n",
      "arcc --- arc\n",
      "eest --- est\n",
      "yooooou --- you\n",
      "wonnnnnnnn --- won\n",
      "ohh --- oh\n"
     ]
    }
   ],
   "source": [
    "# Try remove duplicated chars (not sure about this!!!!!). TODO check fist against vocab?\n",
    "def remove_duplicated_character(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    temp_vocab_dup = []\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        if not word.isalpha():\n",
    "            continue\n",
    "        temp_vocab_dup.append(''.join(ch for ch, _ in itertools.groupby(word)))\n",
    "    temp_vocab_dup = set(temp_vocab_dup)\n",
    "    temp_vocab_dup = temp_vocab_dup.difference(temp_vocab_dup.difference(set(local_vocab)))\n",
    "\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(ch for ch, _ in itertools.groupby(word))\n",
    "        if new_word in temp_vocab_dup:\n",
    "            temp_dict[word] = new_word\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if (k != v) and (v in local_vocab)}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Dup chars (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_duplicated_character)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Disambiguate entities:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 34936 | Known words: 9122\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 34936 | Known words: 9122\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(disambiguate_entitites)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Isolate numbers:\n",
      "Unknown words: 34936 | Known words: 9122\n"
     ]
    }
   ],
   "source": [
    "def isolate_numbers(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if re.compile('[a-zA-Z]').sub('', word) == word:\n",
    "            if re.compile('[0-9]').sub('', word) != word:\n",
    "                temp_dict[word] = word\n",
    "\n",
    "    global_chars_list = list(set([c for line in temp_dict for c in line]))\n",
    "    chars = ''.join([c for c in global_chars_list if not c.isdigit()])\n",
    "    chars_dict = {ord(c):f' {c} ' for c in chars}\n",
    "    temp_dict = {k:place_hold(k) for k in temp_dict}\n",
    "\n",
    "    #texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Isolate numbers:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(isolate_numbers)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Join dashes:\n",
      "Unknown words: 34928 | Known words: 9122\n",
      ".-- --- .-\n",
      "#bitcoin--but --- #bitcoin-but\n",
      "---- --- -\n",
      "-- --- -\n",
      "hahaha--and --- hahaha-and\n",
      "--------- --- -\n",
      "--- --- -\n",
      "--------------- --- -\n",
      "them--china --- them-china\n",
      "----- --- -\n"
     ]
    }
   ],
   "source": [
    "# Join dashes\n",
    "def join_dashes(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('\\-\\-+', '-', word)\n",
    "    temp_dict = {k: v for k, v in temp_dict.items() if k != v}\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Join dashes:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_dashes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 34927 | Known words: 9122\n",
      "fi-na-lly --- finally\n"
     ]
    }
   ],
   "source": [
    "# Try join word (Sloooow)\n",
    "def join_word_letters(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (check_replace(k)) and (Counter(k)['-']>1)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = ''.join(['' if c in '-' else c for c in word])\n",
    "        if (new_word in local_vocab) and (len(new_word)>3):\n",
    "            temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(join_word_letters)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Try Split word:\n",
      "Unknown words: 33543 | Known words: 9244\n",
      "ceo'luktan --- ceo ' luktan\n",
      "d'une --- d ' une\n",
      "a:$33,781 --- a :  $ 33 , 781\n",
      "kazanacaksınız --- kazanacaks ı n ı z\n",
      "#cex.io ---  # cex . io\n",
      "polo.eth --- polo . eth\n",
      "almayı --- almay ı \n",
      "$m ---  $ m\n",
      "🏃 ---  🏃 \n",
      "navalny.eth --- navalny . eth\n"
     ]
    }
   ],
   "source": [
    "# TODO: _ should become ' ' and we should preserve numbers or hashtags\n",
    "# Try Split word\n",
    "def split_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        if len(re.compile('[a-zA-Z0-9\\*]').sub('', word))>0:\n",
    "            chars = re.compile('[a-zA-Z0-9\\*]').sub('', word)\n",
    "            temp_dict[word] = ''.join([' ' + c + ' ' if c in chars else c for c in word])\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Try Split word:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(split_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - L33T (with vocab check):\n",
      "Unknown words: 33542 | Known words: 9244\n",
      "algor1thms --- algorithms\n"
     ]
    }
   ],
   "source": [
    "# L33T vocabulary (SLOW)\n",
    "# https://simple.wikipedia.org/wiki/Leet\n",
    "# Local (only unknown words)\n",
    "def convert_leet(word):\n",
    "    # basic conversion\n",
    "    word = re.sub('0', 'o', word)\n",
    "    word = re.sub('1', 'i', word)\n",
    "    word = re.sub('3', 'e', word)\n",
    "    word = re.sub('\\$', 's', word)\n",
    "    word = re.sub('\\@', 'a', word)\n",
    "    return word\n",
    "\n",
    "def convert_leet_words(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if check_replace(k)]\n",
    "\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        new_word = convert_leet(word)\n",
    "        if (new_word!=word):\n",
    "            if (len(word)>2) and (new_word in local_vocab):\n",
    "                temp_dict[word] = new_word\n",
    "\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - L33T (with vocab check):'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(convert_leet_words)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 33542 | Known words: 9244\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 33500 | Known words: 9245\n",
      "882 --- @NUM[882.0]\n",
      "503 --- @NUM[503.0]\n",
      "886857 --- @NUM[886857.0]\n",
      "47439 --- @NUM[47439.0]\n",
      "36923 --- @NUM[36923.0]\n",
      "33670 --- @NUM[33670.0]\n",
      "33197 --- @NUM[33197.0]\n",
      "429 --- @NUM[429.0]\n",
      "060 --- @NUM[60.0]\n",
      "21598 --- @NUM[21598.0]\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 33499 | Known words: 9245\n",
      "bitstamp --- @bitstamp\n",
      "coinbase --- @coinbase\n",
      "paypal --- @paypal\n",
      "binance --- @binance\n",
      "airdrop --- #airdrop\n",
      "blockchain --- #blockchain\n",
      "bittrex --- @bittrex\n",
      "cryptocurrency --- #cryptocurrency\n",
      "crypto --- #cryptocurrency\n",
      "bitmex --- @bitmex\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 33494 | Known words: 9245\n",
      "comp --- $compound\n",
      "ont --- $ontology\n",
      "wbtc --- $wrapped_bitcoin\n",
      "aave --- $aave\n",
      "ltc --- $litecoin\n",
      "egld --- $elrond_egld\n",
      "ils --- $ils\n",
      "bch --- $bitcoin_cash\n",
      "usdc --- $usd_coin\n",
      "elrond --- $elrond_egld\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 33456 | Known words: 9245\n",
      "$usd_coin --- @CURR[usd_coin]\n",
      "$binance_coin --- @CURR[binance_coin]\n",
      "$thorchain --- @CURR[thorchain]\n",
      "#blockchain --- @HTAG[blockchain]\n",
      "$tron --- @CURR[tron]\n",
      "$bitcoin_cash --- @CURR[bitcoin_cash]\n",
      "$xrp --- @CURR[xrp]\n",
      "$compound --- @HTAG[compound]\n",
      "#hodl --- @HTAG[hodl]\n",
      "$hedera_hashgraph --- @CURR[hedera_hashgraph]\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 33456 | Known words: 9245\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Open Holded words:\n",
      "Unknown words: 33454 | Known words: 9245\n"
     ]
    }
   ],
   "source": [
    "# Remove placeholders\n",
    "def remove_placeholders(texts):\n",
    "    temp_vocab = list(set([c for line in texts for c in line.split()]))\n",
    "    temp_vocab = [k for k in temp_vocab if (not check_replace(k) and k.startswith(WPLACEHOLDER))]\n",
    "    temp_dict = {}\n",
    "    for word in temp_vocab:\n",
    "        temp_dict[word] = re.sub('___', ' ', word[17:-1])\n",
    "    texts = texts.apply(lambda x: ' '.join([temp_dict.get(i, i) for i in x.split()]))\n",
    "    texts = texts.apply(lambda x: ' '.join([i for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Open Holded words:'); check_vocab(texts, local_vocab)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(remove_placeholders)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Multiple form:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "nuevas --- nueva\n",
      "winnings --- winning\n",
      "subscriptions --- subscription\n",
      "chads --- chad\n",
      "collaborates --- collaborate\n",
      "dinos --- dino\n",
      "audits --- audit\n",
      "billionaires --- billionaire\n",
      "grupos --- grupo\n",
      "goldmans --- goldman\n"
     ]
    }
   ],
   "source": [
    "# Search multiple form\n",
    "# Local | example -> flashlights / flashlight -> False / True\n",
    "def search_multiple_form(texts):\n",
    "    temp_vocab = check_vocab(texts, local_vocab, response='unknown_list')\n",
    "    temp_vocab = [k for k in temp_vocab if (k[-1:]=='s') and (len(k)>4)]\n",
    "    temp_dict = {k:k[:-1] for k in temp_vocab if (k[:-1] in local_vocab)}\n",
    "    texts = texts.apply(lambda x: ' '.join([make_dict_cleaning(i,temp_dict) for i in x.split()]))\n",
    "    if verbose: print('#' * 10, 'Step - Multiple form:'); check_vocab(texts, local_vocab);\n",
    "    if verbose: print_dict(temp_dict)\n",
    "    return texts\n",
    "\n",
    "texts = texts.pipe(search_multiple_form)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "########## Step - Custom global word synonyms:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Serialize numbers:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Custom word synonyms:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Custom currency synonyms:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - UserName and Hashtag:\n",
      "Unknown words: 33116 | Known words: 9322\n",
      "########## Step - Hashtag and currency union:\n",
      "Unknown words: 33116 | Known words: 9322\n"
     ]
    }
   ],
   "source": [
    "# Extract entities again and numbers\n",
    "texts = texts\\\n",
    "    .pipe(custom_global_synonyms)\\\n",
    "    .pipe(serialize_numbers)\\\n",
    "    .pipe(custom_synonyms)\\\n",
    "    .pipe(custom_currency_synonyms)\\\n",
    "    .pipe(extract_entities)\\\n",
    "    .pipe(hashtag_currency_union)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted: 0.1362\n",
      "########## Step - Language datection:\n",
      "Unknown words: 22017 | Known words: 8817\n"
     ]
    }
   ],
   "source": [
    "# Cut away non english tweets\n",
    "model = fasttext.load_model('../../data/kaggle/lid.176.ftz')\n",
    "\n",
    "def langcheck(item, min_confidence=0.2):\n",
    "    text = ' '.join([w for w in item.split() if not w.startswith('@')])\n",
    "    if len(text) < 3:\n",
    "        return True\n",
    "    results = dict(zip(*model.predict(text, k=2)))\n",
    "    return results.get('__label__en', 0) > min_confidence\n",
    "\n",
    "mask = texts.parallel_map(langcheck)\n",
    "if verbose: print(f'Deleted: {1 - sum(mask)/len(texts)}')\n",
    "texts = texts[mask]\n",
    "data = data[mask]\n",
    "if verbose: print('#' * 10, 'Step - Language datection:'); check_vocab(texts, local_vocab);"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [
    {
     "data": {
      "text/plain": "                       _id  \\\n0      1356029514439155714   \n1      1356029517123514371   \n2      1356029540590616577   \n3      1356029561264349185   \n4      1356029557757726722   \n...                    ...   \n19995  1356873779323031553   \n19996  1356873843177119744   \n19997  1356873851267874817   \n19998  1356873877733969921   \n19999  1356873897111670784   \n\n                                                                                                                                                                                                          text  \n0                                                                                                                                         edmonton oilers vs ottawa senators . @CURR[bitcoin] @HTAG[betting] -  \n1                                                                      one @CURR[bitcoin] now worth @NUM[33141677.0] usd . market cap @NUM[616963.0] usd billion . based on @HTAG[coindesk] bpi @CURR[bitcoin]  \n2      @USR[dogecoinrich] @USR[dogecoinrise] i have made @NUM[20000.0] usd with @CURR[dogecoin] so far . its not that big but i want to share my profit who doesnt have a chance to board on a train . plea...  \n3      india proposed @HTAG[cryptocurrency] ban has investors nervous , may feed anti - @CURR[bitcoin] narrative @CURR[bitcoin] @HTAG[criptomonedas] @HTAG[trading] @HTAG[volatilidad] @HTAG[pypro] @CURR[b...  \n4                                                                                                                                          what are the coin to rise up ? pld tell me now ! ! ! @CURR[bitcoin]  \n...                                                                                                                                                                                                        ...  \n19995                                                                                                                                                                       @USR[meekmill] plan @CURR[bitcoin]  \n19996                                                                                                                                    @USR[jtjeremybtc] also this guy said that @CURR[bitcoin] going to 0 😁  \n19997  we are still in the early stages of a project with over @NUM[500.0] usd million usd market cap , most projects are pump and dump . @HTAG[golden_ratio_token] is here to stay have a @CURR[the_graph]...  \n19998  riot @HTAG[blockchain] mined 222 @CURR[bitcoin] in the last quarter . and is valued at @NUM[1350000000.0] usd argo @HTAG[blockchain] mined 305 @CURR[bitcoin] in the last quarter . well you see whe...  \n19999                                                                                     @HTAG[aurixapp] @HTAG[aurix] @HTAG[cryptocurrency] @HTAG[aurixexchange] @HTAG[aur] @HTAG[aurixtoken] @HTAG[exchange]  \n\n[17276 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>_id</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1356029514439155714</td>\n      <td>edmonton oilers vs ottawa senators . @CURR[bitcoin] @HTAG[betting] -</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1356029517123514371</td>\n      <td>one @CURR[bitcoin] now worth @NUM[33141677.0] usd . market cap @NUM[616963.0] usd billion . based on @HTAG[coindesk] bpi @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1356029540590616577</td>\n      <td>@USR[dogecoinrich] @USR[dogecoinrise] i have made @NUM[20000.0] usd with @CURR[dogecoin] so far . its not that big but i want to share my profit who doesnt have a chance to board on a train . plea...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1356029561264349185</td>\n      <td>india proposed @HTAG[cryptocurrency] ban has investors nervous , may feed anti - @CURR[bitcoin] narrative @CURR[bitcoin] @HTAG[criptomonedas] @HTAG[trading] @HTAG[volatilidad] @HTAG[pypro] @CURR[b...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1356029557757726722</td>\n      <td>what are the coin to rise up ? pld tell me now ! ! ! @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>19995</th>\n      <td>1356873779323031553</td>\n      <td>@USR[meekmill] plan @CURR[bitcoin]</td>\n    </tr>\n    <tr>\n      <th>19996</th>\n      <td>1356873843177119744</td>\n      <td>@USR[jtjeremybtc] also this guy said that @CURR[bitcoin] going to 0 😁</td>\n    </tr>\n    <tr>\n      <th>19997</th>\n      <td>1356873851267874817</td>\n      <td>we are still in the early stages of a project with over @NUM[500.0] usd million usd market cap , most projects are pump and dump . @HTAG[golden_ratio_token] is here to stay have a @CURR[the_graph]...</td>\n    </tr>\n    <tr>\n      <th>19998</th>\n      <td>1356873877733969921</td>\n      <td>riot @HTAG[blockchain] mined 222 @CURR[bitcoin] in the last quarter . and is valued at @NUM[1350000000.0] usd argo @HTAG[blockchain] mined 305 @CURR[bitcoin] in the last quarter . well you see whe...</td>\n    </tr>\n    <tr>\n      <th>19999</th>\n      <td>1356873897111670784</td>\n      <td>@HTAG[aurixapp] @HTAG[aurix] @HTAG[cryptocurrency] @HTAG[aurixexchange] @HTAG[aur] @HTAG[aurixtoken] @HTAG[exchange]</td>\n    </tr>\n  </tbody>\n</table>\n<p>17276 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['text'] = texts\n",
    "data"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TODO:\n",
    "* numbers\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}